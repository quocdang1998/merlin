// Copyright 2023 quocdang1998
#include "merlin/candy/grad_descent.hpp"

#include <cstdio>

#include "merlin/candy/model.hpp"  // merlin::candy::Model
#include "merlin/utils.hpp"  // merlin::flatten_thread_index, merlin::size_of_block

namespace merlin {

#ifdef __NVCC__

// --------------------------------------------------------------------------------------------------------------------
// GradDescent
// --------------------------------------------------------------------------------------------------------------------

// Update model by gradient on GPU
__cudevice__ void candy::GradDescent::update_gpu(candy::Model * p_model, const double * p_gradient,
                                                 std::uint64_t size) {
    // get thread index and number of threads
    std::uint64_t thread_idx = flatten_thread_index(), n_threads = size_of_block();
    // update
    for (std::uint64_t i_param = thread_idx; i_param < size; i_param += n_threads) {
        auto [param_dim, param_index] = p_model->convert_contiguous(i_param);
        double & param_value = p_model->parameters()[param_dim][param_index];
        param_value -= this->learning_rate_ * p_gradient[i_param];
    }
    __syncthreads();
}

// Update model by gradient value of current thread
__cudevice__ void candy::GradDescent::update_gpu(candy::Model * p_model, const double & gradient,
                                                 const std::uint64_t & param_dim, const std::uint64_t & param_index) {
    double & param_value = p_model->parameters()[param_dim][param_index];
    param_value -= this->learning_rate_ * p_gradient[i_param];
}

// Copy data to shared memory
__cudevice__ void * candy::GradDescent::copy_to_shared_mem(candy::Optimizer * share_ptr, void * data_ptr) const {
    bool check_zeroth_thread = (threadIdx.x == 0) && (threadIdx.y == 0) && (threadIdx.z == 0);
    candy::GradDescent * share_ptr_polymorphic = reinterpret_cast<candy::GradDescent *>(share_ptr);
    // copy learning rate
    if (check_zeroth_thread) {
        share_ptr_polymorphic->learning_rate_ = this->learning_rate_;
    }
    __syncthreads();
    return data_ptr;
}

#ifdef __comment

// --------------------------------------------------------------------------------------------------------------------
// Adam
// --------------------------------------------------------------------------------------------------------------------

// Update model by gradient on GPU
__cudevice__ void candy::Adam::update_gpu(candy::Model * p_model, const double * p_gradient, std::uint64_t size) {
    // get thread index and number of threads
    std::uint64_t thread_idx = flatten_thread_index(), n_threads = size_of_block();
    // calculate first and second moment
    if (thread_idx == 0) {
        this->time_step_ += 1;
    }
    __syncthreads();
    for (std::uint64_t i_param = 0; i_param < size; i_param++) {
        this->register_moments_[2*i_param] *= this->beta_m_;
        this->register_moments_[2*i_param] += (1.0 - this->beta_m_) *  p_gradient[i_param];
        this->register_moments_[2*i_param+1] *= this->beta_v_;
        this->register_moments_[2*i_param+1] += (1.0 - this->beta_v_) * p_gradient[i_param] * p_gradient[i_param];
    }
    __syncthreads();
    // update
    for (std::uint64_t i_param = 0; i_param < size; i_param++) {
        auto [param_dim, param_index] = p_model->convert_contiguous(i_param);
        double & param_value = p_model->parameters()[param_dim][param_index];
        double corrected_first_moment = this->register_moments_[2*i_param];
        corrected_first_moment /= 1.0 - std::pow(this->beta_m_, this->time_step_);
        double corrected_second_moment = this->register_moments_[2*i_param+1];
        corrected_second_moment  /= 1.0 - std::pow(this->beta_v_, this->time_step_);
        double correction = this->learning_rate_ * corrected_first_moment;
        correction /= std::sqrt(corrected_second_moment) + this->bias_;
        param_value -= correction;
    }
    __syncthreads();
}

#endif  // __comment

#endif  // __NVCC__

}  // namespace merlin
