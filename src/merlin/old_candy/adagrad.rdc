// Copyright 2023 quocdang1998
#include "merlin/candy/adagrad.hpp"

#include <cstdio>

#include "merlin/candy/model.hpp"  // merlin::candy::Model
#include "merlin/utils.hpp"  // merlin::flatten_thread_index, merlin::size_of_block

namespace merlin {

#ifdef __NVCC__

// --------------------------------------------------------------------------------------------------------------------
// AdaGrad
// --------------------------------------------------------------------------------------------------------------------

// Constructor on GPU
__cudevice__ candy::AdaGrad::AdaGrad(double learning_rate, double bias, std::uint64_t gradient_size) :
learning_rate_(learning_rate), bias_(bias), cumulative_gradient_norm_(gradient_size, 0.0) {}

// Update model by gradient on GPU
__cudevice__ void candy::AdaGrad::update_gpu(candy::Model * p_model, const double * p_gradient, std::uint64_t size) {
    // get thread index and number of threads
    std::uint64_t thread_idx = flatten_thread_index(), n_threads = size_of_block();
    // loop over each parameter
    for (std::uint64_t i_param = thread_idx; i_param < size; i_param += n_threads) {
        // update gradient norm
        double new_gradient_norm = this->cumulative_gradient_norm_[i_param];
        new_gradient_norm += p_gradient[i_param] * p_gradient[i_param];
        // update parameter
        auto [param_dim, param_index] = p_model->convert_contiguous(i_param);
        double & param_value = p_model->parameters()[param_dim][param_index];
        double correction = this->learning_rate_ * p_gradient[i_param];
        correction /= std::sqrt(new_gradient_norm + this->bias_);
        param_value -= correction;
        // save new norm
        this->cumulative_gradient_norm_[i_param] = new_gradient_norm;
    }
    __syncthreads();
}

// Copy data to shared memory
__cudevice__ void * candy::AdaGrad::copy_to_shared_mem(candy::Optimizer * share_ptr, void * data_ptr) const {
    // copy meta data
    bool check_zeroth_thread = (threadIdx.x == 0) && (threadIdx.y == 0) && (threadIdx.z == 0);
    candy::AdaGrad * share_ptr_polymorphic = reinterpret_cast<candy::AdaGrad *>(share_ptr);
    if (check_zeroth_thread) {
        share_ptr_polymorphic->learning_rate_ = this->learning_rate_;
        share_ptr_polymorphic->bias_ = this->bias_;
        share_ptr_polymorphic->cumulative_gradient_norm_.data() = const_cast<double *>(this->cumulative_gradient_norm_.data());
        share_ptr_polymorphic->cumulative_gradient_norm_.size() = this->cumulative_gradient_norm_.size();
    }
    __syncthreads();
    return data_ptr;
}

#endif  // __NVCC__

}  // namespace merlin
