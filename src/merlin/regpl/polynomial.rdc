// Copyright 2024 quocdang1998
#include "merlin/regpl/polynomial.hpp"

#include "merlin/utils.hpp"  // merlin::prod_elements

namespace merlin {

// ---------------------------------------------------------------------------------------------------------------------
// Polynomial
// ---------------------------------------------------------------------------------------------------------------------

// Get the leaping dimension
__cuhostdev__ static inline std::int64_t leaping_dimension(const std::uint64_t & i_term, const std::uint64_t * shape,
                                                           std::uint64_t ndim) {
    std::int64_t leaping_dim = ndim - 1;
    std::uint64_t cum_prod = 1, nd_index = 0;
    bool not_detected = true;
    for (std::int64_t i_dim = ndim - 1; i_dim >= 0; i_dim--) {
        nd_index = (i_term / cum_prod) % shape[i_dim];
        leaping_dim = ((nd_index == 0) && not_detected) ? leaping_dim - 1 : leaping_dim;
        not_detected &= (nd_index == 0);
        cum_prod *= shape[i_dim];
    }
    return (leaping_dim == -1) ? 0 : leaping_dim;
}

// Evaluate polynomial value at a given point
__cuhostdev__ double regpl::Polynomial::eval(const Point & point, Point & buffer) const noexcept {
    buffer[0] = 0.0;
    buffer[this->ndim() - 1] = 0.0;
    for (std::int64_t i_term = this->size() - 1; i_term >= 0; i_term--) {
        // update the last dimension
        buffer[this->ndim() - 1] *= point[this->ndim() - 1];
        buffer[this->ndim() - 1] += this->coeff_[i_term];
        // get leaping dimension
        std::int64_t leaping_dim = leaping_dimension(i_term, this->order_.data(), this->ndim());
        for (std::int64_t i_dim = this->ndim() - 2; i_dim >= leaping_dim; i_dim--) {
            buffer[i_dim] *= point[i_dim];
            buffer[i_dim] += buffer[i_dim + 1];
            buffer[i_dim + 1] = 0.0;
        }
    }
    return buffer[0];
}

#ifdef __NVCC__

// Copy to shared memory by CUDA block
__cudevice__ void * regpl::Polynomial::copy_by_block(regpl::Polynomial * dest_ptr, void * coeff_data_ptr,
                                                     std::uint64_t thread_idx, std::uint64_t block_size) const {
    // shallow copy of grid node, grid shape and grid vectors
    double * coeff_ptr = reinterpret_cast<double *>(coeff_data_ptr);
    std::uint64_t size = this->size(), ndim = this->ndim();
    if (thread_idx == 0) {
        dest_ptr->coeff_.assign(coeff_ptr, size);
        dest_ptr->order_.resize(ndim);
    }
    __syncthreads();
    // copy data of grid nodes and grid shape
    for (std::uint64_t i_coeff = thread_idx; i_coeff < size; i_coeff += block_size) {
        dest_ptr->coeff_[i_coeff] = this->coeff_[i_coeff];
    }
    __syncthreads();
    for (std::uint64_t i_dim = thread_idx; i_dim < ndim; i_dim += block_size) {
        dest_ptr->order_[i_dim] = this->order_[i_dim];
    }
    __syncthreads();
    return reinterpret_cast<void *>(coeff_ptr + size);
}

// Copy to shared memory by CUDA thread
__cudevice__ void * regpl::Polynomial::copy_by_thread(regpl::Polynomial * dest_ptr, void * coeff_data_ptr) const {
    // shallow copy of grid node, grid shape and grid vectors
    double * coeff_ptr = reinterpret_cast<double *>(coeff_data_ptr);
    std::uint64_t size = this->size(), ndim = this->ndim();
    dest_ptr->coeff_.assign(coeff_ptr, size);
    dest_ptr->order_.resize(ndim);
    // copy data of grid nodes and grid shape
    for (std::uint64_t i_coeff = 0; i_coeff < size; i_coeff++) {
        dest_ptr->coeff_[i_coeff] = this->coeff_[i_coeff];
    }
    for (std::uint64_t i_dim = 0; i_dim < ndim; i_dim++) {
        dest_ptr->order_[i_dim] = this->order_[i_dim];
    }
    return reinterpret_cast<void *>(coeff_ptr + size);
}

#endif  // __NVCC__

}  // namespace merlin
