// Copyright 2022 quocdang1998
#include "merlin/interpolant/lagrange.hpp"

#include "merlin/array/array.hpp"  // merlin::array::Array
#include "merlin/array/parcel.hpp"  // merlin::array::Parcel
#include "merlin/interpolant/cartesian_grid.hpp"  // merlin::interpolant::CartesianGrid
#include "merlin/logger.hpp"  // FAILURE
#include "merlin/utils.hpp"  // merlin::contiguous_to_ndim_idx

namespace merlin {

// Calculate multi-dim index with some skipped dimension
__cuhostdev__ static intvec contiguous_to_ndim_idx_collapsed(const std::uint64_t & index, const intvec & full_shape,
                                                             const intvec & collapsed_index,
                                                             std::uint64_t * data_ptr = nullptr) {
    // calculate non collapsed ndim
    std::uint64_t non_collapsed_ndim = 0;
    for (std::uint64_t i_dim = 0; i_dim < full_shape.size(); i_dim++) {
        if (collapsed_index[i_dim] != UINT64_MAX) {
            non_collapsed_ndim++;
        }
    }
    // initialize index vector
    intvec index_;
    if (data_ptr != nullptr) {
        index_.assign(data_ptr, non_collapsed_ndim);
    } else {
        index_ = intvec(non_collapsed_ndim);
    }
    // trivial case : all dimensions collapsed
    if (non_collapsed_ndim == 0) {
        return index_;  // empty shape vector
    }
    // save index of non collapsed dimension
    for (std::uint64_t i_dim = 0, i_collapsed = 0; i_dim < full_shape.size(); i_dim++) {
        if (collapsed_index[i_dim] != UINT64_MAX) {
            index_[i_collapsed++] = i_dim;
        }
    }
    // calculate index vector
    std::uint64_t cum_prod = 1;
    std::uint64_t index_current_dim = index % full_shape[index_[non_collapsed_ndim-1]];
    for (std::int64_t i = non_collapsed_ndim-2; i >= 0; i--) {
        cum_prod *= full_shape[index_[i+1]];
        index_[i+1] = index_current_dim;
        index_current_dim = (index / cum_prod) % full_shape[index_[i]];
    }
    index_[0] = index_current_dim;
    return index_;
}

// Evaluate Lagrange interpolation on a full Cartesian grid using CPU
__cuhostdev__ double interpolant::eval_lagrange_single_core(const interpolant::CartesianGrid & grid,
                                                            const array::NdData & coeff, const Vector<double> & x,
                                                            std::uint64_t * collapsed_index_data,
                                                            std::uint64_t * i_point_data) {
    // Type casting
    #ifdef __CUDA_ARCH__
    const array::Parcel * p_coeff = static_cast<const array::Parcel *>(&coeff);
    #else
    const array::Array * p_coeff = static_cast<const array::Array *>(&coeff);
    #endif  // __CUDA_ARCH__
    // check if point x lies on a hyperplane passing through a node of the grid
    std::uint64_t ndim = grid.ndim();
    intvec collapsed_index;
    if (collapsed_index_data == nullptr) {
        collapsed_index = intvec(ndim, UINT64_MAX);
    } else {
        collapsed_index.assign(collapsed_index_data, ndim);
        for (std::uint64_t i_dim = 0; i_dim < ndim; i_dim++) {
            collapsed_index_data[i_dim] = UINT64_MAX;
        }
    }
    std::uint64_t collapsed_dim_count = 0;
    for (std::uint64_t i_dim = 0; i_dim < ndim; i_dim++) {
        const Vector<double> & nodes = grid.grid_vectors()[i_dim];
        for (std::uint64_t i_node = 0; i_node < nodes.size(); i_node++) {
            if (x[i_dim] == nodes[i_node]) {
                collapsed_index[i_dim] = i_node;
                collapsed_dim_count++;
                break;
            }
        }
    }
    // calculate common factor for points on grid line
    double common_factor = 1.0;
    double product_point = 1.0;
    for (std::uint64_t i_dim = 0; i_dim < ndim; i_dim++) {
        const Vector<double> & nodes = grid.grid_vectors()[i_dim];
        if (collapsed_index[i_dim] != UINT64_MAX) {
            // calculate common_factor if point on grid line
            for (std::uint64_t i_node = 0; i_node < nodes.size(); i_node++) {
                if (i_node == collapsed_index[i_dim]) {
                    continue;
                }
                common_factor *= (nodes[collapsed_index[i_dim]] - nodes[i_node]);
            }
        } else {
            // calculate product of point wrt. every node
            for (std::uint64_t i_node = 0; i_node < nodes.size(); i_node++) {
                product_point *= (x[i_dim] - nodes[i_node]);
            }
        }
    }
    // calculate shape of collapsed coefficient array
    intvec non_collapse_shape(ndim - collapsed_dim_count);
    std::uint64_t non_collapse_size = 1;
    for (std::uint64_t i_dim = 0, i_non_collapse = 0; i_dim < ndim; i_dim++) {
        if (collapsed_index[i_dim] == UINT64_MAX) {
            non_collapse_shape[i_non_collapse] = coeff.shape()[i_dim];
            non_collapse_size *= non_collapse_shape[i_non_collapse];
            i_non_collapse++;
        }
    }
    // loop over each uncollapsed point of the coeff array
    double result = 0.0;
    intvec sub_coeff_index(non_collapse_shape.size());
    intvec whole_coeff_index(collapsed_index);
    for (std::uint64_t i_point = 0; i_point < non_collapse_size; i_point++) {
        contiguous_to_ndim_idx(i_point, non_collapse_shape, sub_coeff_index.data());
        // index wrt. uncollapsed coeff array
        for (std::uint64_t i_dim = 0, i_dim_non_collapsed = 0; i_dim < ndim; i_dim++) {
            if (collapsed_index[i_dim] == UINT64_MAX) {
                whole_coeff_index[i_dim] = sub_coeff_index[i_dim_non_collapsed++];
            }
        }
        // calculate denominator
        double denominator = 1.0;
        for (std::uint64_t i_dim = 0; i_dim < ndim; i_dim++) {
            if (collapsed_index[i_dim] == UINT64_MAX) {
                denominator *= x[i_dim] - grid.grid_vectors()[i_dim][whole_coeff_index[i_dim]];
            }
        }
        result += (*p_coeff)[whole_coeff_index] / denominator;
    }
    result *= common_factor * product_point;
    return result;
}

}  // namespace merlin
