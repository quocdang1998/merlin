// Copyright 2022 quocdang1998
#include "merlin/interpolant/lagrange.hpp"

#include "merlin/array/array.hpp"  // merlin::array::Array
#include "merlin/array/parcel.hpp"  // merlin::array::Parcel
#include "merlin/interpolant/cartesian_grid.hpp"  // merlin::interpolant::CartesianGrid
#include "merlin/logger.hpp"  // FAILURE
#include "merlin/utils.hpp"  // merlin::contiguous_to_ndim_idx

namespace merlin {

// Calculate multi-dim index with some skipped dimension
__cuhostdev__ static void contiguous_to_ndim_idx_collapsed(const std::uint64_t & index, const intvec & full_shape,
                                                             const intvec & collapsed_index,
                                                             std::uint64_t * data_ptr) {
    // calculate non collapsed ndim
    std::uint64_t non_collapsed_ndim = 0, i_dim_last_collapsed = UINT64_MAX;
    for (std::uint64_t i_dim = 0; i_dim < full_shape.size(); i_dim++) {
        if (collapsed_index[i_dim] == UINT64_MAX) {
            non_collapsed_ndim++;
            i_dim_last_collapsed = i_dim;
        }
    }
    // initialize index vector
    // trivial case : all dimensions collapsed
    if (non_collapsed_ndim == 0) {
        return;  // empty shape vector
    }
    // calculate index vector
    std::uint64_t cum_prod = 1, current_dim_shape = full_shape[i_dim_last_collapsed];
    data_ptr[i_dim_last_collapsed] = index % current_dim_shape;
    for (std::int64_t i = i_dim_last_collapsed-1; i >= 0; i--) {
        if (collapsed_index[i] != UINT64_MAX) {
            continue;
        }
        cum_prod *= current_dim_shape;
        current_dim_shape = full_shape[i];
        data_ptr[i] = (index / cum_prod) % current_dim_shape;
    }
}

// Evaluate Lagrange interpolation on a full Cartesian grid using CPU
__cuhostdev__ double interpolant::eval_lagrange_single_core(const interpolant::CartesianGrid & grid,
                                                            const array::NdData & coeff, const Vector<double> & x,
                                                            std::uint64_t * collapsed_index_data,
                                                            std::uint64_t * whole_coeff_data) {
    // Type casting
    #ifdef __CUDA_ARCH__
    const array::Parcel * p_coeff = static_cast<const array::Parcel *>(&coeff);
    #else
    const array::Array * p_coeff = static_cast<const array::Array *>(&coeff);
    #endif  // __CUDA_ARCH__
    // check if point x lies on a hyperplane passing through a node of the grid
    std::uint64_t ndim = grid.ndim();
    const intvec & shape = p_coeff->shape();
    intvec collapsed_index;
    if (collapsed_index_data == nullptr) {
        collapsed_index = intvec(ndim, UINT64_MAX);
    } else {
        collapsed_index.assign(collapsed_index_data, ndim);
        for (std::uint64_t i_dim = 0; i_dim < ndim; i_dim++) {
            collapsed_index_data[i_dim] = UINT64_MAX;
        }
    }
    std::uint64_t collapsed_dim_count = 0;
    for (std::uint64_t i_dim = 0; i_dim < ndim; i_dim++) {
        const Vector<double> & nodes = grid.grid_vectors()[i_dim];
        for (std::uint64_t i_node = 0; i_node < nodes.size(); i_node++) {
            if (x[i_dim] == nodes[i_node]) {
                collapsed_index[i_dim] = i_node;
                collapsed_dim_count++;
                break;
            }
        }
    }
    // calculate common factor for points on grid line
    double common_factor = 1.0;
    double product_point = 1.0;
    for (std::uint64_t i_dim = 0; i_dim < ndim; i_dim++) {
        const Vector<double> & nodes = grid.grid_vectors()[i_dim];
        if (collapsed_index[i_dim] != UINT64_MAX) {
            // calculate common_factor if point on grid line
            for (std::uint64_t i_node = 0; i_node < nodes.size(); i_node++) {
                if (i_node == collapsed_index[i_dim]) {
                    continue;
                }
                common_factor *= (nodes[collapsed_index[i_dim]] - nodes[i_node]);
            }
        } else {
            // calculate product of point wrt. every node
            for (std::uint64_t i_node = 0; i_node < nodes.size(); i_node++) {
                product_point *= (x[i_dim] - nodes[i_node]);
            }
        }
    }
    // calculate shape of collapsed coefficient array
    std::uint64_t non_collapse_size = 1;
    for (std::uint64_t i_dim = 0; i_dim < ndim; i_dim++) {
        if (collapsed_index[i_dim] == UINT64_MAX) {
            non_collapse_size *= p_coeff->shape()[i_dim];
        }
    }
    // initialize whole_coeff_index
    intvec whole_coeff_index;
    if (whole_coeff_data == nullptr) {
        whole_coeff_index = intvec(collapsed_index);
    } else {
        whole_coeff_index.assign(whole_coeff_data, ndim);
        for (std::uint64_t i_dim = 0; i_dim < ndim; i_dim++) {
            whole_coeff_index[i_dim] = collapsed_index[i_dim];
        }
    }
    // loop over each uncollapsed point of the coeff array
    double result = 0.0;
    for (std::uint64_t i_point = 0; i_point < non_collapse_size; i_point++) {
        contiguous_to_ndim_idx_collapsed(i_point, shape, collapsed_index, whole_coeff_index.data());
        // calculate denominator
        double denominator = 1.0;
        for (std::uint64_t i_dim = 0; i_dim < ndim; i_dim++) {
            if (collapsed_index[i_dim] == UINT64_MAX) {
                denominator *= x[i_dim] - grid.grid_vectors()[i_dim][whole_coeff_index[i_dim]];
            }
        }
        result += (*p_coeff)[whole_coeff_index] / denominator;
    }
    result *= common_factor * product_point;
    return result;
}

}  // namespace merlin
