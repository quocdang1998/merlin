// Copyright 2022 quocdang1998
#include "merlin/interpolant/lagrange.hpp"

#include <tuple>  // std::get

#include "merlin/array/parcel.hpp"  // merlin::array::Parcel
#include "merlin/array/nddata.hpp"  // merlin::array::max_allowed_dim
#include "merlin/cuda/memory.hpp"  // merlin::cuda::copy_class_to_shared_mem
#include "merlin/interpolant/cartesian_grid.hpp"  // merlin::interpolant::CartesianGrid
#include "merlin/utils.hpp"  // merlin::flatten_kernel_index, merlin::get_block_count, merlin::contiguous_to_ndim_idx

namespace merlin {

// --------------------------------------------------------------------------------------------------------------------
// GPU kernel wrapper
// --------------------------------------------------------------------------------------------------------------------

// Calculate interpolation on a given index
__cudevice__ static void calc_lagrange_coefficients(const interpolant::CartesianGrid * p_grid,
                                                    const array::Parcel * p_value, array::Parcel * p_coeff,
                                                    std::uintptr_t free_shared_mem) {
    // get array index from contiguous index
    std::uint64_t ndim = p_grid->ndim();
    const intvec & shape = p_value->shape();
    // calculate the denomiantor (product of diferences of node values)
    std::uint64_t size = p_value->size();
    std::uint64_t i_thread = flatten_thread_index();
    std::uint64_t n_thread = size_of_block();
    // std::uint64_t index_data[array::max_allowed_dim];
    std::uint64_t * index_data = reinterpret_cast<std::uint64_t *>(free_shared_mem) + i_thread * ndim;
    for (std::uint64_t i = i_thread; i < size; i += n_thread) {
        intvec index = contiguous_to_ndim_idx(i, shape, index_data);
        double denominator = 1.0;
        for (std::uint64_t i_dim = 0; i_dim < ndim; i_dim++) {
            for (std::uint64_t i_node = 0; i_node < shape[i_dim]; i_node++) {
                // skip for repeating index
                if (i_node == index[i_dim]) {
                    continue;
                }
                denominator *= p_grid->grid_vectors()[i_dim][index[i_dim]] - p_grid->grid_vectors()[i_dim][i_node];
            }
        }
        double result = p_value->operator[](index) / static_cast<double>(denominator);
        p_coeff->operator[](index) = result;
    }
    __syncthreads();
}

// Call CUDA kernel calculating coefficients on GPU
__global__ static void calc_lagrange_kernel(const interpolant::CartesianGrid * p_grid, const array::Parcel * p_value,
                                            array::Parcel * p_coeff) {
    // copy meta data to shared memory
    extern __shared__ char share_ptr[];
    auto shared_mem_tuple = cuda::copy_class_to_shared_mem(share_ptr, *p_grid, *p_value, *p_coeff);
    interpolant::CartesianGrid * p_grid_shared = std::get<0>(shared_mem_tuple);
    array::Parcel * p_value_shared = std::get<1>(shared_mem_tuple);
    array::Parcel * p_coeff_shared = std::get<2>(shared_mem_tuple);
    std::uintptr_t free_shared_mem = reinterpret_cast<std::uintptr_t>(p_coeff_shared) + sizeof(array::Parcel)
                                     + 2*(p_coeff_shared->ndim())*sizeof(std::uint64_t);
    // perform the calculation
    calc_lagrange_coefficients(p_grid_shared, p_value_shared, p_coeff_shared, free_shared_mem);
}

// Call calculation kernel on GPU
void interpolant::call_lagrange_coeff_kernel(const interpolant::CartesianGrid * p_grid, const array::Parcel * p_value,
                                             array::Parcel * p_coeff, std::uint64_t shared_mem_size,
                                             std::uintptr_t stream_ptr, std::uint64_t n_thread) {
    ::cudaStream_t cuda_stream = reinterpret_cast<::cudaStream_t>(stream_ptr);
    calc_lagrange_kernel<<<1, n_thread, shared_mem_size, cuda_stream>>>(p_grid, p_value, p_coeff);
}

}  // namespace merlin
