// Copyright 2022 quocdang1998
#include "merlin/intpl/newton.hpp"

#include "merlin/array/parcel.hpp"  // merlin::array::Parcel
#include "merlin/cuda/memory.hpp"  // merlin::cuda::copy_class_to_shared_mem
#include "merlin/intpl/cartesian_grid.hpp"  // merlin::intpl::CartesianGrid
#include "merlin/utils.hpp"  // merlin::flatten_kernel_index, merlin::get_block_count, merlin::contiguous_to_ndim_idx

namespace merlin {

// Calculate divide difference by parallel on each matrix
__cudevice__ static void calc_newton_coeffs_1(const Vector<double> & grid_vector, array::Parcel & coeff,
                                              const std::uint64_t & i_dim,
                                              const std::uint64_t & thrd_idx, const std::uint64_t & n_th,
                                              const std::uint64_t & size_previous_dims, intvec & shape_previous_dims,
                                              const std::uint64_t & size_divdiff_space, intvec & shape_divdiff_space,
                                              std::uint64_t * point_index_k_data, intvec & point_index_k) {
    // loop on each previous dims point
    for (std::int64_t i_previous_dims = thrd_idx; i_previous_dims < size_previous_dims; i_previous_dims += n_th) {
        point_index_k.assign(point_index_k_data, coeff.ndim());
        contiguous_to_ndim_idx(i_previous_dims, shape_previous_dims, point_index_k_data);
        // loop on indices of current dim for divide difference
        for (std::uint64_t i = 1; i < coeff.shape()[i_dim]; i++) {
            for (std::uint64_t k = coeff.shape()[i_dim]-1; k >= i; k--) {
                // loop on each point in divdiff space
                for (std::uint64_t i_divdiff = 0; i_divdiff < size_divdiff_space; i_divdiff++) {
                    contiguous_to_ndim_idx(i_divdiff, shape_divdiff_space, point_index_k_data+i_dim+1);
                    point_index_k_data[i_dim] = k;
                    double divdiff_result = coeff[point_index_k];
                    point_index_k_data[i_dim] = k-1;
                    divdiff_result -= coeff[point_index_k];
                    divdiff_result /= grid_vector[k] - grid_vector[k-i];
                    point_index_k_data[i_dim] = k;
                    coeff[point_index_k] = divdiff_result;
                }
            }
        }
    }
    __syncthreads();
}

// Calculate divide difference by parallel on each entry
__cudevice__ static void calc_newton_coeffs_2(const Vector<double> & grid_vector, array::Parcel & coeff,
                                              const std::uint64_t & i_dim,
                                              const std::uint64_t & thrd_idx, const std::uint64_t & n_th,
                                              const std::uint64_t & size_previous_dims, intvec & shape_previous_dims,
                                              const std::uint64_t & size_divdiff_space, intvec & shape_divdiff_space,
                                              std::uint64_t * point_index_k_data, intvec & point_index_k) {
    // loop on each previous dims point
    for (std::int64_t i_previous_dims = 0; i_previous_dims < size_previous_dims; i_previous_dims++) {
        point_index_k.assign(point_index_k_data, coeff.ndim());
        contiguous_to_ndim_idx(i_previous_dims, shape_previous_dims, point_index_k_data);
        // loop on indices of current dim for divide difference
        for (std::uint64_t i = 1; i < coeff.shape()[i_dim]; i++) {
            for (std::uint64_t k = coeff.shape()[i_dim]-1; k >= i; k--) {
                // loop on each point in divdiff space
                for (std::uint64_t i_divdiff = thrd_idx; i_divdiff < size_divdiff_space; i_divdiff += n_th) {
                    contiguous_to_ndim_idx(i_divdiff, shape_divdiff_space, point_index_k_data+i_dim+1);
                    point_index_k_data[i_dim] = k;
                    double divdiff_result = coeff[point_index_k];
                    point_index_k_data[i_dim] = k-1;
                    divdiff_result -= coeff[point_index_k];
                    divdiff_result /= grid_vector[k] - grid_vector[k-i];
                    point_index_k_data[i_dim] = k;
                    coeff[point_index_k] = divdiff_result;
                }
                __syncthreads();
            }
        }
    }
}

// Calculate divide difference algorithm
__cudevice__ static void calc_newton_coeffs_single_core(const intpl::CartesianGrid & grid, array::Parcel & coeff,
                                                        std::uintptr_t free_shared_mem) {
    // get thread index and n_thread
    std::uint64_t thrd_idx = flatten_thread_index();
    std::uint64_t n_th = size_of_block();
    // initilaize data intvec
    std::uint64_t * point_index_k_data = reinterpret_cast<std::uint64_t *>(free_shared_mem) + thrd_idx * grid.ndim();
    intvec point_index_k;
    // loop on each dimension
    for (std::uint64_t i_dim = 0; i_dim < coeff.ndim(); i_dim++) {
        // get grid vector at current diemnsion
        const Vector<double> & grid_vector = grid.grid_vectors()[grid.ndim() - coeff.ndim() + i_dim];
        // get shape and size of previous dimensions
        intvec shape_previous_dims;
        shape_previous_dims.assign(const_cast<std::uint64_t *>(coeff.shape().begin()), i_dim);
        std::uint64_t size_previous_dims = prod_elements(shape_previous_dims);
        // get shape and size of divdiff subspace
        intvec shape_divdiff_space;
        shape_divdiff_space.assign(const_cast<std::uint64_t *>(coeff.shape().begin())+i_dim+1,
                                   const_cast<std::uint64_t *>(coeff.shape().end()));
        std::uint64_t size_divdiff_space = prod_elements(shape_divdiff_space);
        if (size_previous_dims > 2*n_th) {
            calc_newton_coeffs_1(grid_vector, coeff, i_dim, thrd_idx, n_th, size_previous_dims, shape_previous_dims,
                                 size_divdiff_space, shape_divdiff_space, point_index_k_data, point_index_k);
        } else {
            calc_newton_coeffs_2(grid_vector, coeff, i_dim, thrd_idx, n_th, size_previous_dims, shape_previous_dims,
                                 size_divdiff_space, shape_divdiff_space, point_index_k_data, point_index_k);
        }
    }
}

// Calculate divide difference by a single thread on GPU
__global__ static void calc_single_core_kernel(const intpl::CartesianGrid * p_grid, array::Parcel * p_coeff) {
    // copy meta data to GPU
    extern __shared__ char share_ptr[];
    auto shared_mem_tuple = cuda::copy_class_to_shared_mem(share_ptr, *p_grid, *p_coeff);
    intpl::CartesianGrid * p_grid_shared = std::get<1>(shared_mem_tuple);
    array::Parcel * p_coeff_shared = std::get<2>(shared_mem_tuple);
    std::uintptr_t free_shared_mem = reinterpret_cast<std::uintptr_t>(std::get<0>(shared_mem_tuple));
    calc_newton_coeffs_single_core(*p_grid_shared, *p_coeff_shared, free_shared_mem);
}

// Call coefficient calculation on single core
void intpl::call_newton_coeff_kernel(const intpl::CartesianGrid * p_grid, array::Parcel * p_coeff,
                                           std::uint64_t shared_mem_size, std::uintptr_t stream_ptr,
                                           std::uint64_t n_thread) {
    ::cudaStream_t cuda_stream = reinterpret_cast<::cudaStream_t>(stream_ptr);
    calc_single_core_kernel<<<1, n_thread, shared_mem_size, cuda_stream>>>(p_grid, p_coeff);
}

// Call CUDA kernel calculating coefficients on GPU
__global__ static void eval_newton_kernel(const intpl::CartesianGrid * p_grid, const array::Parcel * p_coeff,
                                          const array::Parcel * p_points, Vector<double> * p_result) {
    // copy meta data to shared memory
    extern __shared__ char share_ptr[];
    auto shared_mem_tuple = cuda::copy_class_to_shared_mem(share_ptr, *p_grid, *p_coeff, *p_points);
    intpl::CartesianGrid * p_grid_shared = std::get<1>(shared_mem_tuple);
    array::Parcel * p_coeff_shared = std::get<2>(shared_mem_tuple);
    array::Parcel * p_points_shared = std::get<3>(shared_mem_tuple);
    std::uintptr_t free_shared_mem = reinterpret_cast<std::uintptr_t>(std::get<0>(shared_mem_tuple));
    // perform the calculation
    std::uint64_t ndim = p_grid->ndim();
    std::uint64_t n_point = p_result->size();
    std::uint64_t i_thread = flatten_thread_index();
    std::uint64_t n_thread = size_of_block();
    free_shared_mem += i_thread * ndim * (sizeof(std::uint64_t) + sizeof(double));
    std::uint64_t * iterator_data = reinterpret_cast<std::uint64_t *>(free_shared_mem);
    double * cummulative_register_data = reinterpret_cast<double *>(iterator_data + ndim);
    for (std::uint64_t i_point = i_thread; i_point < n_point; i_point += n_thread) {
        Vector<double> point;
        std::uintptr_t point_ptr = reinterpret_cast<std::uintptr_t>(p_points->data()) + i_point*p_points->strides()[0];
        point.assign(reinterpret_cast<double *>(point_ptr), ndim);
        (*p_result)[i_point] = intpl::eval_newton_single_core(*p_grid, *p_coeff, point, iterator_data,
                                                                    cummulative_register_data);
    }
    __syncthreads();
}

// Call the GPU kernel evaluating Newton interpolation
void intpl::call_newton_eval_kernel(const intpl::CartesianGrid * p_grid, const array::Parcel * p_coeff,
                                    const array::Parcel * p_points, Vector<double> * p_result,
                                    std::uint64_t shared_mem_size, std::uintptr_t stream_ptr, std::uint64_t n_thread) {
    ::cudaStream_t cuda_stream = reinterpret_cast<::cudaStream_t>(stream_ptr);
    eval_newton_kernel<<<1, n_thread, shared_mem_size, cuda_stream>>>(p_grid, p_coeff, p_points, p_result);
}

}  // namespace merlin
