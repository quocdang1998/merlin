// Copyright 2024 quocdang1998
#include "merlin/candy/train/gpu_trainer.hpp"

#include "merlin/array/parcel.hpp"       // merlin::array::Parcel
#include "merlin/candy/gradient.hpp"     // merlin::candy::Gradient
#include "merlin/candy/loss.hpp"         // merlin::candy::rmse_gpu
#include "merlin/candy/model.hpp"        // merlin::candy::Model
#include "merlin/candy/optimizer.hpp"    // merlin::candy::Optimizer
#include "merlin/cuda/copy_helpers.hpp"  // merlin::cuda::copy_objects
#include "merlin/utils.hpp"              // merlin::flatten_block_index, merlin::flatten_thread_index,
                                         // merlin::is_finite, merlin::size_of_block

namespace merlin {

// ---------------------------------------------------------------------------------------------------------------------
// Utility
// ---------------------------------------------------------------------------------------------------------------------

// Update based on error
__cudevice__ inline void update_go_on(double & priori_error, double & posteriori_error, const double & threshold,
                                      bool & go_on, bool & local_go_on, const std::uint64_t & thread_idx) {
    if (thread_idx == 0) {
        if (is_finite(posteriori_error)) {
            go_on = (std::abs(priori_error - posteriori_error) / posteriori_error > threshold);
        } else {
            go_on = false;
        }
    }
    __syncthreads();
    local_go_on = go_on;
}

// ---------------------------------------------------------------------------------------------------------------------
// Kernels
// ---------------------------------------------------------------------------------------------------------------------

// Dry-run
__global__ static void kernel_dry_run(candy::Model * p_model, candy::Optimizer * p_optmz, const array::Parcel * p_data,
                                      std::uint64_t * p_cases, double * p_error, std::uint64_t * p_count,
                                      candy::TrialPolicy policy, candy::TrainMetric metric) {
    // get block index, thread index and number of threads per block
    std::uint64_t block_index = flatten_block_index();
    std::uint64_t thread_idx = flatten_thread_index(), block_size = size_of_block();
    // copy objects on shared memory
    std::uint64_t i_case = p_cases[block_index];
    double * error = p_error + block_index * policy.sum();
    std::uint64_t & count = p_count[block_index];
    candy::Model & model = p_model[i_case];
    candy::Optimizer & optmz = p_optmz[i_case];
    const array::Parcel & data = p_data[i_case];
    extern __shared__ char shared_mem[];
    auto [grad_mem, p_model_shr, p_optmz_shr, p_data_shr] = cuda::copy_objects(shared_mem, thread_idx, block_size,
                                                                               model, optmz, data);
    // assign data to gradient
    candy::Gradient gradient(reinterpret_cast<double *>(grad_mem), p_model_shr->num_params(), metric);
    // initialize memory for index
    __shared__ unsigned long long normal_count;
    __shared__ double saved_error;
    __shared__ bool break_condition;
    Index index_mem;
    index_mem.fill(0);
    // calculate initial error
    candy::rmse_gpu(p_model_shr, p_data_shr, &saved_error, &normal_count, thread_idx, block_size, index_mem);
    if (thread_idx == 0) {
        error[0] = saved_error;
        count = 1;
    }
    __syncthreads();
    // discarded phase
    std::uint64_t start = 0;
    std::uint64_t loops = policy.discarded();
    for (std::uint64_t iter = 1; iter < loops; iter++) {
        gradient.calc_by_gpu(*p_model_shr, *p_data_shr, thread_idx, block_size, index_mem);
        p_optmz_shr->update_gpu(*p_model_shr, gradient, start + iter, thread_idx, block_size);
        candy::rmse_gpu(p_model_shr, p_data_shr, &saved_error, &normal_count, thread_idx, block_size, index_mem);
        if (thread_idx == 0) {
            error[start + iter] = saved_error;
            break_condition = !is_finite(error[start + iter]);
            count += (break_condition) ? 0 : 1;
        }
        __syncthreads();
        if (break_condition) {
            break;
        }
    }
    loops = (count == start + loops) ? (policy.strict()) : 0;
    start += policy.discarded();
    // strictly descent phase
    for (std::uint64_t iter = 0; iter < loops; iter++) {
        gradient.calc_by_gpu(*p_model_shr, *p_data_shr, thread_idx, block_size, index_mem);
        p_optmz_shr->update_gpu(*p_model_shr, gradient, start + iter, thread_idx, block_size);
        candy::rmse_gpu(p_model_shr, p_data_shr, &saved_error, &normal_count, thread_idx, block_size, index_mem);
        if (thread_idx == 0) {
            error[start + iter] = saved_error;
            break_condition = !is_finite(error[start + iter]) ||
                              (error[start + iter] >= strict_max_ratio * error[start + iter - 1]);
            count += (break_condition) ? 0 : 1;
        }
        __syncthreads();
        if (break_condition) {
            break;
        }
    }
    loops = (count == start + loops) ? (policy.loose()) : 0;
    start += policy.strict();
    // loose phase
    for (std::uint64_t iter = 0; iter < loops; iter++) {
        gradient.calc_by_gpu(*p_model_shr, *p_data_shr, thread_idx, block_size, index_mem);
        p_optmz_shr->update_gpu(*p_model_shr, gradient, start + iter, thread_idx, block_size);
        candy::rmse_gpu(p_model_shr, p_data_shr, &saved_error, &normal_count, thread_idx, block_size, index_mem);
        if (thread_idx == 0) {
            error[start + iter] = saved_error;
            break_condition = !is_finite(error[start + iter]) ||
                              (error[start + iter] >= loose_max_ratio * error[start + iter - 1]);
            count += (break_condition) ? 0 : 1;
        }
        __syncthreads();
        if (break_condition) {
            break;
        }
    }
}

// Launch CUDA kernel dry-running
void candy::train::launch_dry_run(candy::Model * p_model, candy::Optimizer * p_optmz, const array::Parcel * p_data,
                                  std::uint64_t * p_cases, double * p_error, std::uint64_t * p_count,
                                  std::uint64_t size, candy::TrialPolicy policy, candy::TrainMetric metric,
                                  std::uint64_t block_size, std::uint64_t shared_mem_size, std::uintptr_t stream_ptr) {
    ::cudaStream_t cuda_stream = reinterpret_cast<::cudaStream_t>(stream_ptr);
    kernel_dry_run<<<size, block_size, shared_mem_size, cuda_stream>>>(p_model, p_optmz, p_data, p_cases, p_error,
                                                                       p_count, policy, metric);
}

// Train a model until a given threshold is met
__global__ static void kernel_update_until(candy::Model * p_model, candy::Optimizer * p_optmz,
                                           const array::Parcel * p_data, std::uint64_t rep, double threshold,
                                           candy::TrainMetric metric) {
    // get block index, thread index and number of threads per block
    std::uint64_t block_index = flatten_block_index();
    std::uint64_t thread_idx = flatten_thread_index(), block_size = size_of_block();
    // copy objects on shared memory
    candy::Model & model = p_model[block_index];
    candy::Optimizer & optmz = p_optmz[block_index];
    const array::Parcel & data = p_data[block_index];
    extern __shared__ char shared_mem[];
    auto [grad_mem, p_model_shr, p_optmz_shr, p_data_shr] = cuda::copy_objects(shared_mem, thread_idx, block_size,
                                                                               model, optmz, data);
    // assign data to gradient
    candy::Gradient gradient(reinterpret_cast<double *>(grad_mem), p_model_shr->num_params(), metric);
    // initialize error variable
    __shared__ double priori_error;
    __shared__ double posteriori_error;
    __shared__ unsigned long long normal_count;
    __shared__ bool go_on;
    bool local_go_on = true;
    if (thread_idx == 0) {
        priori_error = 0.0;
        go_on = true;
    }
    __syncthreads();
    // calculate posteriori error before training
    Index index_mem;
    index_mem.fill(0);
    candy::rmse_gpu(p_model_shr, p_data_shr, &posteriori_error, &normal_count, thread_idx, block_size, index_mem);
    std::uint64_t step = 1;
    do {
        // copy posterioiri error -> priori error
        if (thread_idx == 0) {
            priori_error = posteriori_error;
        }
        __syncthreads();
        // repeat training process
        for (std::uint64_t i = 0; i < rep; i++) {
            gradient.calc_by_gpu(*p_model_shr, *p_data_shr, thread_idx, block_size, index_mem);
            p_optmz->update_gpu(*p_model_shr, gradient, step + i, thread_idx, block_size);
        }
        candy::rmse_gpu(p_model_shr, p_data_shr, &posteriori_error, &normal_count, thread_idx, block_size, index_mem);
        update_go_on(priori_error, posteriori_error, threshold, go_on, local_go_on, thread_idx);
        step += rep;
    } while (local_go_on);
    // copy model and optimizer data from shared memory back to global memory
    p_model_shr->copy_by_block(&model, model.data(), thread_idx, block_size);
    p_optmz_shr->copy_by_block(&optmz, optmz.dynamic_data(), thread_idx, block_size);
}

// Launch CUDA kernel training a model until a given threshold is met
void candy::train::launch_update_until(candy::Model * p_model, candy::Optimizer * p_optmz, const array::Parcel * p_data,
                                       std::uint64_t size, std::uint64_t rep, double threshold,
                                       std::uint64_t block_size, candy::TrainMetric metric,
                                       std::uint64_t shared_mem_size, std::uintptr_t stream_ptr) {
    ::cudaStream_t cuda_stream = reinterpret_cast<::cudaStream_t>(stream_ptr);
    kernel_update_until<<<size, block_size, shared_mem_size, cuda_stream>>>(p_model, p_optmz, p_data, rep, threshold,
                                                                            metric);
}

// Train a model for a fixed number of iterations
__global__ static void kernel_update_for(candy::Model * p_model, candy::Optimizer * p_optmz,
                                         const array::Parcel * p_data, std::uint64_t max_iter,
                                         candy::TrainMetric metric) {
    // get block index, thread index and number of threads per block
    std::uint64_t block_index = flatten_block_index();
    std::uint64_t thread_idx = flatten_thread_index(), block_size = size_of_block();
    // copy objects on shared memory
    candy::Model & model = p_model[block_index];
    candy::Optimizer & optmz = p_optmz[block_index];
    const array::Parcel & data = p_data[block_index];
    extern __shared__ char shared_mem[];
    auto [grad_mem, p_model_shr, p_optmz_shr, p_data_shr] = cuda::copy_objects(shared_mem, thread_idx, block_size,
                                                                               model, optmz, data);
    // assign data to gradient
    candy::Gradient gradient(reinterpret_cast<double *>(grad_mem), p_model_shr->num_params(), metric);
    // initialize cache memory
    Index index_mem;
    index_mem.fill(0);
    // training loop
    for (std::uint64_t i = 1; i <= max_iter; i++) {
        gradient.calc_by_gpu(*p_model_shr, *p_data_shr, thread_idx, block_size, index_mem);
        p_optmz->update_gpu(*p_model_shr, gradient, i, thread_idx, block_size);
    }
    // copy model and optimizer data from shared memory back to global memory
    p_model_shr->copy_by_block(&model, model.data(), thread_idx, block_size);
    p_optmz_shr->copy_by_block(&optmz, optmz.dynamic_data(), thread_idx, block_size);
}

// Launch CUDA kernel training a model for a fixed number of iterations
void candy::train::launch_update_for(candy::Model * p_model, candy::Optimizer * p_optmz, const array::Parcel * p_data,
                                     std::uint64_t size, std::uint64_t max_iter, std::uint64_t block_size,
                                     candy::TrainMetric metric, std::uint64_t shared_mem_size,
                                     std::uintptr_t stream_ptr) {
    ::cudaStream_t cuda_stream = reinterpret_cast<::cudaStream_t>(stream_ptr);
    kernel_update_for<<<size, block_size, shared_mem_size, cuda_stream>>>(p_model, p_optmz, p_data, max_iter, metric);
}

// Reconstruct CP-model
__global__ static void kernel_reconstruct(candy::Model * p_model, array::Parcel * p_data) {
    // get block index, thread index and number of threads per block
    std::uint64_t block_index = flatten_block_index();
    std::uint64_t thread_idx = flatten_thread_index(), block_size = size_of_block();
    // copy objects on shared memory
    candy::Model & model = p_model[block_index];
    array::Parcel & data = p_data[block_index];
    extern __shared__ char shared_mem[];
    auto [remain, p_model_shr, p_data_shr] = cuda::copy_objects(shared_mem, thread_idx, block_size, model, data);
    Index index;
    index.fill(0);
    for (std::uint64_t c_index = thread_idx; c_index < p_data_shr->size(); c_index += block_size) {
        contiguous_to_ndim_idx(c_index, p_data_shr->shape().data(), p_data_shr->ndim(), index.data());
        p_data_shr->operator[](index) = p_model_shr->eval(index);
    }
}

// Launch CUDA kernel reconstructing data
void candy::train::launch_reconstruct(candy::Model * p_model, array::Parcel * p_data, std::uint64_t size,
                                      std::uint64_t block_size, std::uint64_t shared_mem_size,
                                      std::uintptr_t stream_ptr) {
    ::cudaStream_t cuda_stream = reinterpret_cast<::cudaStream_t>(stream_ptr);
    kernel_reconstruct<<<size, block_size, shared_mem_size, cuda_stream>>>(p_model, p_data);
}

// Calculate error
__global__ static void kernel_get_error(candy::Model * p_model, array::Parcel * p_data, double * p_error) {
    // get block index, thread index and number of threads per block
    std::uint64_t block_index = flatten_block_index();
    std::uint64_t thread_idx = flatten_thread_index(), block_size = size_of_block();
    // copy objects on shared memory
    extern __shared__ char shared_mem[];
    candy::Model & model = p_model[block_index];
    array::Parcel & data = p_data[block_index];
    double & rmse = p_error[2 * block_index];
    double & rmae = p_error[2 * block_index + 1];
    auto [remain, p_model_shr, p_data_shr] = cuda::copy_objects(shared_mem, thread_idx, block_size, model, data);
    // initialize memory for result on shred memory
    __shared__ double rmse_shr;
    __shared__ double rmae_shr;
    __shared__ unsigned long long normal_count;
    Index index_mem;
    index_mem.fill(0);
    candy::rmse_gpu(p_model_shr, p_data_shr, &rmse_shr, &normal_count, thread_idx, block_size, index_mem);
    candy::rmae_gpu(p_model_shr, p_data_shr, &rmae_shr, &normal_count, thread_idx, block_size, index_mem);
    // save results to global memory
    if (thread_idx == 0) {
        rmse = rmse_shr;
        rmae = rmae_shr;
    }
    __syncthreads();
}

// Launch CUDA kernel calculating error
void candy::train::launch_get_error(candy::Model * p_model, array::Parcel * p_data, double * p_error,
                                    std::uint64_t size, std::uint64_t block_size, std::uint64_t shared_mem_size,
                                    std::uintptr_t stream_ptr) {
    ::cudaStream_t cuda_stream = reinterpret_cast<::cudaStream_t>(stream_ptr);
    kernel_get_error<<<size, block_size, shared_mem_size, cuda_stream>>>(p_model, p_data, p_error);
}

}  // namespace merlin
