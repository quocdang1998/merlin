// Copyright 2023 quocdang1998
#include "merlin/candy/trainer.hpp"

#include "merlin/array/parcel.hpp"     // merlin::array::Parcel
#include "merlin/candy/gradient.hpp"   // merlin::candy::Gradient
#include "merlin/candy/loss.hpp"       // merlin::candy::rmse_gpu
#include "merlin/candy/model.hpp"      // merlin::candy::Model
#include "merlin/candy/optimizer.hpp"  // merlin::candy::Optimizer
#include "merlin/cuda/memory.hpp"      // merlin::cuda::copy_objects
#include "merlin/utils.hpp"            // merlin::flatten_thread_index, merlin::size_of_block, merlin::is_normal

namespace merlin {

// ---------------------------------------------------------------------------------------------------------------------
// Utility
// ---------------------------------------------------------------------------------------------------------------------

// Update based on error
__cudevice__ inline void update_go_on(double & priori_error, double & posteriori_error, const double & threshold,
                                      bool & go_on, bool & local_go_on, const std::uint64_t & thread_idx) {
    if (thread_idx == 0) {
        if (is_normal(posteriori_error)) {
            go_on = (std::abs(priori_error - posteriori_error) / posteriori_error > threshold);
        } else {
            go_on = false;
        }
    }
    __syncthreads();
    local_go_on = go_on;
}

// Training kernel
__global__ static void train_kernel(candy::Model * p_model, const array::Parcel * p_data, candy::Optimizer * p_optmz,
                                    candy::TrainMetric metric, std::uint64_t rep, double threshold) {
    // get thread index and number of threads
    std::uint64_t thread_idx = flatten_thread_index(), block_size = size_of_block();
    // copy objects on shared memory
    extern __shared__ char shared_mem[];
    auto [grad_mem, p_model_shr, p_data_shr, p_optmz_shr] = cuda::copy_objects(shared_mem, *p_model, *p_data, *p_optmz);
    // assign data to gradient
    candy::Gradient gradient(reinterpret_cast<double *>(grad_mem), p_model_shr->num_params(), metric);
    // initialize error variable
    __shared__ double priori_error;
    __shared__ double posteriori_error;
    __shared__ unsigned long long normal_count;
    __shared__ bool go_on;
    bool local_go_on = true;
    if (thread_idx == 0) {
        priori_error = 0.0;
        go_on = true;
    }
    __syncthreads();
    // calculate posteriori error before training
    Index index_mem;
    index_mem.fill(0);
    candy::rmse_gpu(p_model_shr, p_data_shr, &posteriori_error, &normal_count, thread_idx, block_size, index_mem);
    std::uint64_t step = 1;
    do {
        // copy posterioiri error -> priori error
        if (thread_idx == 0) {
            priori_error = posteriori_error;
        }
        __syncthreads();
        // repeat training process
        for (std::uint64_t i = 0; i < rep; i++) {
            gradient.calc_by_gpu(*p_model_shr, *p_data_shr, thread_idx, block_size, index_mem);
            p_optmz->update_gpu(*p_model_shr, gradient, step + i, thread_idx, block_size);
        }
        candy::rmse_gpu(p_model_shr, p_data_shr, &posteriori_error, &normal_count, thread_idx, block_size, index_mem);
        update_go_on(priori_error, posteriori_error, threshold, go_on, local_go_on, thread_idx);
        step += rep;
    } while (local_go_on);
    // copy model and optimizer data from shared memory back to global memory
    p_model_shr->copy_by_block(p_model, p_model + 1, thread_idx, block_size);
    p_optmz_shr->copy_by_block(p_optmz, p_optmz + 1, thread_idx, block_size);
}

// Train a model using GPU parallelism
void candy::train_by_gpu(candy::Model * p_model, const array::Parcel * p_data, candy::Optimizer * p_optimizer,
                         candy::TrainMetric metric, std::uint64_t rep, std::uint64_t n_threads, double threshold,
                         std::uint64_t shared_mem_size, cuda::Stream & stream) {
    ::cudaStream_t cuda_stream = reinterpret_cast<::cudaStream_t>(stream.get_stream_ptr());
    train_kernel<<<1, n_threads, shared_mem_size, cuda_stream>>>(p_model, p_data, p_optimizer, metric, rep, threshold);
}

// Error kernel
__global__ void error_kernel(candy::Model * p_model, const array::Parcel * p_data, double * p_rmse, double * p_rmae) {
    // get thread index and number of threads
    std::uint64_t thread_idx = flatten_thread_index(), block_size = size_of_block();
    // copy objects on shared memory
    extern __shared__ char shared_mem[];
    auto [p_end, p_model_shr, p_data_shr] = cuda::copy_objects(shared_mem, *p_model, *p_data);
    // initialize error variable
    __shared__ double rmse_shr;
    __shared__ double rmae_shr;
    __shared__ unsigned long long normal_count;
    Index index_mem;
    index_mem.fill(0);
    candy::rmse_gpu(p_model_shr, p_data_shr, &rmse_shr, &normal_count, thread_idx, block_size, index_mem);
    candy::rmae_gpu(p_model_shr, p_data_shr, &rmae_shr, &normal_count, thread_idx, block_size, index_mem);
    // save to memory
    if (thread_idx == 0) {
        *p_rmse = rmse_shr;
        *p_rmae = rmae_shr;
    }
    __syncthreads();
}

// Calculate error using GPU parallelism
void candy::error_by_gpu(candy::Model * p_model, const array::Parcel * p_data, double * p_rmse, double * p_rmae,
                         std::uint64_t n_threads, std::uint64_t shared_mem_size, cuda::Stream & stream) {
    ::cudaStream_t cuda_stream = reinterpret_cast<::cudaStream_t>(stream.get_stream_ptr());
    error_kernel<<<1, n_threads, shared_mem_size, cuda_stream>>>(p_model, p_data, p_rmse, p_rmae);
}

__global__ void dryrun_kernel(candy::Model * p_model, const array::Parcel * p_data, candy::Optimizer * p_optmz,
                              candy::TrainMetric metric, double * error, std::uint64_t * count,
                              std::uint64_t max_iter) {
    // get thread index and number of threads
    std::uint64_t thread_idx = flatten_thread_index(), block_size = size_of_block();
    // copy objects on shared memory
    extern __shared__ char shared_mem[];
    auto [grad_mem, p_model_shr, p_data_shr, p_optmz_shr] = cuda::copy_objects(shared_mem, *p_model, *p_data, *p_optmz);
    // assign data to gradient
    candy::Gradient gradient(reinterpret_cast<double *>(grad_mem), p_model_shr->num_params(), metric);
    // calculate initial error
    __shared__ unsigned long long normal_count;
    __shared__ double saved_error;
    __shared__ bool break_condition;
    Index index_mem;
    index_mem.fill(0);
    candy::rmse_gpu(p_model_shr, p_data_shr, &saved_error, &normal_count, thread_idx, block_size, index_mem);
    if (thread_idx == 0) {
        error[0] = saved_error;
        *count = 1;
    }
    __syncthreads();
    // repeatedly iterate until a surge in the error detected
    for (std::uint64_t iter = 1; iter < max_iter; iter++) {
        gradient.calc_by_gpu(*p_model_shr, *p_data_shr, thread_idx, block_size, index_mem);
        p_optmz->update_gpu(*p_model_shr, gradient, iter, thread_idx, block_size);
        candy::rmse_gpu(p_model_shr, p_data_shr, &saved_error, &normal_count, thread_idx, block_size, index_mem);
        if (thread_idx == 0) {
            error[iter] = saved_error;
            break_condition = !is_normal(saved_error) || (saved_error / error[iter - 1] >= 1.0 + 1e-10);
        }
        __syncthreads();
        if (break_condition) {
            break;
        }
        if (thread_idx == 0) {
            *count = iter + 1;
        }
        __syncthreads();
    }
}

// Dry-run the gradient update algorithm using GPU parallelism
void candy::dryrun_by_gpu(candy::Model * p_model, const array::Parcel * p_data, candy::Optimizer * p_optimizer,
                          candy::TrainMetric metric, std::uint64_t n_threads, double * error, std::uint64_t * count,
                          std::uint64_t max_iter, std::uint64_t shared_mem_size, cuda::Stream & stream) {
    ::cudaStream_t cuda_stream = reinterpret_cast<::cudaStream_t>(stream.get_stream_ptr());
    dryrun_kernel<<<1, n_threads, shared_mem_size, cuda_stream>>>(p_model, p_data, p_optimizer, metric, error, count,
                                                                  max_iter);
}

// Reconstruct CP-model
__global__ void reconstruct_kernel(candy::Model * p_model, array::Parcel * p_data) {
    // get thread index and number of threads
    std::uint64_t thread_idx = flatten_thread_index(), block_size = size_of_block();
    // copy objects on shared memory
    extern __shared__ char shared_mem[];
    auto [remain, p_model_shr, p_data_shr] = cuda::copy_objects(shared_mem, *p_model, *p_data);
    Index index;
    index.fill(0);
    for (std::uint64_t c_index = thread_idx; c_index < p_data_shr->size(); c_index += block_size) {
        contiguous_to_ndim_idx(c_index, p_data_shr->shape().data(), p_data_shr->ndim(), index.data());
        p_data_shr->operator[](index) = p_model_shr->eval(index);
    }
}

// Reconstruct CP-model using GPU parallelism
void candy::reconstruct_by_gpu(candy::Model * p_model, array::Parcel * p_data, std::uint64_t n_threads,
                               std::uint64_t shared_mem_size, cuda::Stream & stream) {
    ::cudaStream_t cuda_stream = reinterpret_cast<::cudaStream_t>(stream.get_stream_ptr());
    reconstruct_kernel<<<1, n_threads, shared_mem_size, cuda_stream>>>(p_model, p_data);
}

}  // namespace merlin
