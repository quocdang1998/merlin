// Copyright 2023 quocdang1998
#include "merlin/candy/trainer.hpp"

#include "merlin/array/parcel.hpp"     // merlin::array::Parcel
#include "merlin/candy/gradient.hpp"   // merlin::candy::Gradient
#include "merlin/candy/loss.hpp"       // merlin::candy::rmse_gpu
#include "merlin/candy/model.hpp"      // merlin::candy::Model
#include "merlin/candy/optimizer.hpp"  // merlin::candy::Optimizer
#include "merlin/cuda/memory.hpp"      // merlin::cuda::copy_objects
#include "merlin/utils.hpp"            // merlin::flatten_thread_index, merlin::size_of_block, merlin::is_normal

namespace merlin {

// ---------------------------------------------------------------------------------------------------------------------
// Utility
// ---------------------------------------------------------------------------------------------------------------------

// Update based on error
__device__ inline void update_go_on(double & priori_error, double & posteriori_error, const double & threshold,
                                    bool & go_on, bool & local_go_on, const std::uint64_t & thread_idx) {
    if (thread_idx == 0) {
        if (is_normal(posteriori_error)) {
            go_on = (std::abs(priori_error - posteriori_error) / posteriori_error > threshold);
        } else {
            go_on = false;
        }
    }
    __syncthreads();
    local_go_on = go_on;
}

// training kernel
__global__ static void train_kernel(candy::Model * p_model, const array::Parcel * p_data, candy::Optimizer * p_optmz,
                                    candy::TrainMetric metric, std::uint64_t rep, double threshold) {
    // get thread index and number of threads
    std::uint64_t thread_idx = flatten_thread_index(), block_size = size_of_block();
    // copy objects on shared memory
    extern __shared__ char shared_mem[];
    auto [grad_mem, p_model_shr, p_data_shr, p_optmz_shr] = cuda::copy_objects(shared_mem, *p_model, *p_data, *p_optmz);
    // assign data to gradient
    candy::Gradient gradient(reinterpret_cast<double *>(grad_mem), p_model_shr->num_params(), metric);
    // initialize error variable
    __shared__ double priori_error;
    __shared__ double posteriori_error;
    __shared__ unsigned long long normal_count;
    __shared__ bool go_on;
    bool local_go_on = true;
    if (thread_idx == 0) {
        priori_error = 0.0;
        go_on = true;
    }
    __syncthreads();
    // calculate posteriori error before training
    Index index_mem;
    index_mem.fill(0);
    candy::rmse_gpu(p_model_shr, p_data_shr, &posteriori_error, &normal_count, thread_idx, block_size, index_mem);
    do {
        // copy posterioiri error -> priori error
        if (thread_idx == 0) {
            priori_error = posteriori_error;
        }
        __syncthreads();
        // repeat training process
        for (std::uint64_t i = 0; i < rep; i++) {
            gradient.calc_by_gpu(*p_model_shr, *p_data_shr, thread_idx, block_size, index_mem);
            p_optmz->update_gpu(*p_model_shr, gradient, thread_idx, block_size);
        }
        candy::rmse_gpu(p_model_shr, p_data_shr, &posteriori_error, &normal_count, thread_idx, block_size, index_mem);
        update_go_on(priori_error, posteriori_error, threshold, go_on, local_go_on, thread_idx);
    } while (local_go_on);
    // copy model data from shared memory back to global memory
    p_model_shr->copy_by_block(p_model, p_model + 1, thread_idx, block_size);
}

// Train a model using GPU parallelism
void candy::train_by_gpu(candy::Model * p_model, const array::Parcel * p_data, candy::Optimizer * p_optimizer,
                         candy::TrainMetric metric, std::uint64_t rep, std::uint64_t n_threads, std::uint64_t ndim,
                         double threshold, std::uint64_t shared_mem_size, cuda::Stream & stream) {
    // launch the calculation asynchronously on GPU
    ::cudaStream_t cuda_stream = reinterpret_cast<::cudaStream_t>(stream.get_stream_ptr());
    train_kernel<<<1, n_threads, shared_mem_size, cuda_stream>>>(p_model, p_data, p_optimizer, metric, rep, threshold);
}

}  // namespace merlin
