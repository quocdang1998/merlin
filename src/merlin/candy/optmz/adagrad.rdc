// Copyright 2023 quocdang1998
#include "merlin/candy/optmz/adagrad.hpp"

#include "merlin/candy/model.hpp"     // merlin::candy::Model
#include "merlin/candy/gradient.hpp"  // merlin::candy::Gradient

namespace merlin {

// ---------------------------------------------------------------------------------------------------------------------
// AdaGrad
// ---------------------------------------------------------------------------------------------------------------------

#ifdef __NVCC__

// Update model inside a GPU parallel region
__cudevice__ void candy::optmz::AdaGrad::update_gpu(candy::Model & model, const candy::Gradient & grad,
                                                    std::uint64_t thread_idx, std::uint64_t n_threads) noexcept {
    for (std::uint64_t i_param = thread_idx; i_param < this->grad_history.size(); i_param += n_threads) {
        double & param = model[i_param];
        this->grad_history[i_param] += grad.value()[i_param] * grad.value()[i_param];
        double correction = this->learning_rate * grad.value()[i_param];
        correction /= std::sqrt(this->grad_history[i_param] + this->bias);
        param -= correction;
    }
    __syncthreads();
}

// Copy model to pre-allocated memory region by current CUDA block of threads
__cudevice__ void * candy::optmz::AdaGrad::copy_by_block(candy::optmz::AdaGrad * dest_ptr, void * dynamic_data_ptr,
                                                         std::uint64_t thread_idx, std::uint64_t block_size) const {
    if (thread_idx == 0) {
        dest_ptr->learning_rate = this->learning_rate;
        dest_ptr->bias = this->bias;
        dest_ptr->grad_history.data() = reinterpret_cast<double *>(dynamic_data_ptr);
        dest_ptr->grad_history.size() = this->grad_history.size();
    }
    __syncthreads();
    for(std::uint64_t i = thread_idx; i < dest_ptr->grad_history.size(); i += block_size) {
        dest_ptr->grad_history[i] = this->grad_history[i];
    }
    __syncthreads();
}

// Copy object to a pre-allocated memory region by a single GPU threads
__cudevice__ void * candy::optmz::copy_adagrad_by_thread(void * dest_ptr, const void * src_ptr,
                                                         void * dynamic_data_ptr) {
    candy::optmz::AdaGrad * dptr = reinterpret_cast<candy::optmz::AdaGrad *>(dest_ptr);
    const candy::optmz::AdaGrad * sptr = reinterpret_cast<const candy::optmz::AdaGrad *>(src_ptr);
    dptr->learning_rate = sptr->learning_rate;
    dptr->bias = sptr->bias;
    dptr->grad_history.data() = reinterpret_cast<double *>(dynamic_data_ptr);
    dptr->grad_history.size() = sptr->grad_history.size();
    for(std::uint64_t i = 0; i < dptr->grad_history.size(); i++) {
        dptr->grad_history[i] = sptr->grad_history[i];
    }
}

#endif  // __NVCC__

}  // namespace merlin
