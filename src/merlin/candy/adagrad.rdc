// Copyright 2023 quocdang1998
#include "merlin/candy/adagrad.hpp"

#include <cstdio>

#include "merlin/candy/model.hpp"  // merlin::candy::Model
#include "merlin/utils.hpp"  // merlin::flatten_thread_index, merlin::size_of_block

namespace merlin {

#ifdef __NVCC__

// --------------------------------------------------------------------------------------------------------------------
// AdaGrad
// --------------------------------------------------------------------------------------------------------------------

// Constructor on GPU
__cudevice__ candy::AdaGrad::AdaGrad(double learning_rate, double bias, std::uint64_t gradient_size) :
learning_rate_(learning_rate), bias_(bias), cumulative_gradient_norm_(gradient_size, 0.0) {}

// Update model by gradient on GPU
__cudevice__ void candy::AdaGrad::update_gpu(candy::Model * p_model, const double * p_gradient, std::uint64_t size) {
    // get thread index and number of threads
    std::uint64_t thread_idx = flatten_thread_index(), n_threads = size_of_block();
    // loop over each parameter
    for (std::uint64_t i_param = thread_idx; i_param < size; i_param += n_threads) {
        // update gradient norm
        double new_gradient_norm = this->cumulative_gradient_norm_[i_param];
        new_gradient_norm += p_gradient[i_param] * p_gradient[i_param];
        // update parameter
        auto [param_dim, param_index] = p_model->convert_contiguous(i_param);
        double & param_value = p_model->parameters()[param_dim][param_index];
        double correction = this->learning_rate_ * p_gradient[i_param];
        correction /= std::sqrt(new_gradient_norm + this->bias_);
        param_value -= correction;
        // save new norm
        this->cumulative_gradient_norm_[i_param] = new_gradient_norm;
    }
    __syncthreads();
}

#endif  // __NVCC__

}  // namespace merlin
