// Copyright 2023 quocdang1998
#include "merlin/candy/loss.hpp"

#include <cmath>  // std::abs, std::sqrt

#include "merlin/array/parcel.hpp"  // merlin::array::Parcel
#include "merlin/candy/model.hpp"   // merlin::candy::Model
#include "merlin/utils.hpp"         // merlin::contiguous_to_ndim_idx, merlin::is_normal, merlin::is_finite

namespace merlin {

// ---------------------------------------------------------------------------------------------------------------------
// RMSE
// ---------------------------------------------------------------------------------------------------------------------

#ifdef __NVCC__

// CUDA function calculating relative mean square error with GPU parallelism
__cudevice__ void candy::rmse_gpu(const candy::Model * p_model, const array::Parcel * p_data, double * p_result,
                                  unsigned long long * p_count, std::uint64_t thread_idx, std::uint64_t block_size,
                                  Index & index_mem) noexcept {
    // initialize result
    std::uint64_t thread_count = 0;
    double thread_rmse = 0.0;
    if (thread_idx == 0) {
        *p_result = 0.0;
        *p_count = 0;
    }
    __syncthreads();
    // summing on all points
    for (std::uint64_t i_point = thread_idx; i_point < p_data->size(); i_point += block_size) {
        contiguous_to_ndim_idx(i_point, p_data->shape().data(), p_data->ndim(), index_mem.data());
        double x_data = (*p_data)[index_mem];
        if (!is_normal(x_data)) {
            continue;
        }
        thread_count += 1;
        double x_model = p_model->eval(index_mem);
        double rel_err = (x_model - x_data) / x_data;
        thread_rmse += rel_err * rel_err;
    }
    __syncthreads();
    // accumulate
    ::atomicAdd_block(p_count, static_cast<unsigned long long>(thread_count));
    ::atomicAdd_block(p_result, thread_rmse);
    if (thread_idx == 0) {
        *p_result = std::sqrt(*p_result / *p_count);
    }
    __syncthreads();
}

#endif  // __NVCC__

// ---------------------------------------------------------------------------------------------------------------------
// RMAE
// ---------------------------------------------------------------------------------------------------------------------

#ifdef __NVCC__

// CUDA function calculating relative mean square error with GPU parallelism
__cudevice__ void candy::rmae_gpu(const candy::Model * p_model, const array::Parcel * p_data, double * p_result,
                                  unsigned long long * p_count, std::uint64_t thread_idx, std::uint64_t block_size,
                                  Index & index_mem) noexcept {
    // initialize result
    std::uint64_t thread_count = 0;
    double thread_rmae = 0.0;
    if (thread_idx == 0) {
        *p_result = 0.0;
        *p_count = 0;
    }
    // summing on all points
    for (std::uint64_t i_point = thread_idx; i_point < p_data->size(); i_point += block_size) {
        contiguous_to_ndim_idx(i_point, p_data->shape().data(), p_data->ndim(), index_mem.data());
        double x_data = (*p_data)[index_mem];
        if (!is_finite(x_data)) {
            continue;
        }
        thread_count += 1;
        double x_model = p_model->eval(index_mem);
        double rel_err = std::abs(x_model - x_data) / x_data;
        thread_rmae = (thread_rmae < rel_err) ? rel_err : thread_rmae;
    }
    __syncthreads();
    // accumulate
    ::atomicAdd_block(p_count, static_cast<unsigned long long>(thread_count));
    ::atomicMax_block(reinterpret_cast<long long int *>(p_result), __double_as_longlong(thread_rmae));
}

#endif  // __NVCC__

}  // namespace merlin
