// Copyright 2023 quocdang1998
#include "merlin/candy/model.hpp"

#include "merlin/logger.hpp"  // CUHDERR
#include "merlin/utils.hpp"  // merlin::flatten_thread_index, merlin::size_of_block

namespace merlin {

// Get shape
__cuhostdev__ intvec candy::Model::get_model_shape(std::uint64_t * data_ptr) const noexcept {
    intvec result;
    if (data_ptr != nullptr) {
        result.assign(data_ptr, this->ndim());
    } else {
        result = intvec(this->parameters_.size());
    }
    for (std::uint64_t i_dim = 0; i_dim < this->ndim(); i_dim++) {
        result[i_dim] = this->parameters_[i_dim].size();
    }
    return result;
}

// Number of elements
__cuhostdev__ std::uint64_t candy::Model::size(void) const noexcept {
    std::uint64_t size = 0;
    for (std::uint64_t i_dim = 0; i_dim < this->ndim(); i_dim++) {
        size += this->parameters_[i_dim].size();
    }
    return size;
}

// Get dimension and index from contiguous index
__cuhostdev__ std::array<std::uint64_t, 2> candy::Model::convert_contiguous(std::uint64_t index) const noexcept {
    std::uint64_t dimension = 0;
    std::int64_t index_ = index;
    for (std::uint64_t i_dim = 0; i_dim < this->parameters_.size(); i_dim++) {
        index_ -= this->parameters_[i_dim].size();
        dimension += (index_ >= 0) ? 1 : 0;
        index = (index_ >= 0) ? index_ : index;
    }
    return {dimension, index};
}

// Get reference to element from flattened index
__cuhostdev__ const double & candy::Model::get(std::uint64_t index) const noexcept {
    auto [dimension, idx] = this->convert_contiguous(index);
    return this->parameters_[dimension][idx];
}

// Set value of element from flattened index
__cuhostdev__ void candy::Model::set(std::uint64_t index, double value) noexcept {
    auto [dimension, idx] = this->convert_contiguous(index);
    this->parameters_[dimension][idx] = value;
}

// Evaluate result of the model at a given index
__cuhostdev__ double candy::Model::eval(const intvec & index) const noexcept {
    double result = 0.0;
    for (std::uint64_t r = 0; r < this->rank_; r++) {
        double rank_eval = 1.0;
        for (std::uint64_t i_dim = 0; i_dim < this->ndim(); i_dim++) {
            rank_eval *= this->get(i_dim, index[i_dim], r);
        }
        result += rank_eval;
    }
    return result;
}

#ifdef __NVCC__

// Copy to shared memory
__cudevice__ void * candy::Model::copy_to_shared_mem(candy::Model * share_ptr, void * parameters_data_ptr) const {
    /*
    // shallow copy of parameters vector
    bool check_zeroth_thread = (threadIdx.x == 0) && (threadIdx.y == 0) && (threadIdx.z == 0);
    if (check_zeroth_thread) {
        share_ptr->parameters_.data() = reinterpret_cast<floatvec *>(parameters_data_ptr);
        share_ptr->parameters_.size() = this->ndim();
        share_ptr->rank_ = this->rank_;
        std::uintptr_t dptr = reinterpret_cast<std::uintptr_t>(parameters_data_ptr);
        for (std::uint64_t i_dim = 0; i_dim < this->ndim(); i_dim++) {
            floatvec * p_parameter_dim = reinterpret_cast<floatvec *>(dptr);
            p_parameter_dim->data() = const_cast<double *>(this->parameters_[i_dim].data());
            p_parameter_dim->size() = this->parameters_[i_dim].size();
            dptr = reinterpret_cast<std::uintptr_t>(p_parameter_dim+1);
        }
    }
    floatvec * result = reinterpret_cast<floatvec *>(parameters_data_ptr);
    return reinterpret_cast<void *>(result + this->ndim());
    */
    // copy of parameter vector pointer
    std::uint64_t thread_idx = flatten_thread_index(), block_size = size_of_block();
    if (thread_idx == 0) {
        share_ptr->parameters_.data() = reinterpret_cast<floatvec *>(parameters_data_ptr);
        share_ptr->parameters_.size() = this->ndim();
        share_ptr->rank_ = this->rank_;
    }
    __syncthreads();
    // assign pointer to each floatvec
    std::uintptr_t parameter_data = reinterpret_cast<std::uintptr_t>(parameters_data_ptr);
    parameter_data += sizeof(floatvec) * share_ptr->ndim();
    double * floatvecs_data = reinterpret_cast<double *>(parameter_data);
    for (std::uint64_t i_dim = 0; i_dim < share_ptr->ndim(); i_dim++) {
        std::uint64_t dim_size = this->parameters_[i_dim].size();
        if (thread_idx == 0) {
            share_ptr->parameters_[i_dim].data() = floatvecs_data;
            share_ptr->parameters_[i_dim].size() = dim_size;
        }
        __syncthreads();
        floatvecs_data += dim_size;
    }
    // copy parameter value
    double * param_data = reinterpret_cast<double *>(parameter_data);
    std::uint64_t model_size = this->size();
    for (std::uint64_t i_param = 0; i_param < model_size; i_param += block_size) {
        param_data[i_param] = this->parameters_[0].data()[i_param];
    }
    __syncthreads();
    return reinterpret_cast<void *>(floatvecs_data);
}

// Copy to shared memory
__cudevice__ void * candy::Model::copy_to_shared_mem_single(candy::Model * share_ptr,
                                                            void * parameters_data_ptr) const {
    // shallow copy of parameters vector
    share_ptr->parameters_.data() = reinterpret_cast<floatvec *>(parameters_data_ptr);
    share_ptr->parameters_.size() = this->ndim();
    share_ptr->rank_ = this->rank_;
    std::uintptr_t dptr = reinterpret_cast<std::uintptr_t>(parameters_data_ptr);
    for (std::uint64_t i_dim = 0; i_dim < this->ndim(); i_dim++) {
        floatvec * p_parameter_dim = reinterpret_cast<floatvec *>(dptr);
        p_parameter_dim->data() = const_cast<double *>(this->parameters_[i_dim].data());
        p_parameter_dim->size() = this->parameters_[i_dim].size();
        dptr = reinterpret_cast<std::uintptr_t>(p_parameter_dim+1);
    }
    floatvec * result = reinterpret_cast<floatvec *>(parameters_data_ptr);
    return reinterpret_cast<void *>(result + this->ndim());
}

#endif  // __NVCC__

}  // namespace merlin
