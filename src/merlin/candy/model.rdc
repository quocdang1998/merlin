// Copyright 2023 quocdang1998
#include "merlin/candy/model.hpp"

#include <algorithm>  // std::all_of

#include "merlin/utils.hpp"  // merlin::index_in_subsequence

namespace merlin {

// Get dimension, index on that dimension and rank corresponding to a flatten index
__cuhostdev__ std::array<std::uint64_t, 3> candy::Model::get_param_index(std::uint64_t i_param) const noexcept {
    std::uint64_t rank = i_param / this->rstride_;
    std::uint64_t rank_index = i_param % this->rstride_;
    auto [i_dim, index] = index_in_subsequence(rank_index, this->shape_.data(), this->ndim());
    return std::array<std::uint64_t, 3>({i_dim, index, rank});
}

// Evaluate result of the model at a given index
__cuhostdev__ double candy::Model::eval(const Index & index) const noexcept {
    double result = 0;
    const double * p_parameter = this->parameters_.data();
    for (std::uint64_t r = 0; r < this->rank_; r++) {
        double rank_eval = 1.0;
        for (std::uint64_t i_dim = 0; i_dim < this->ndim(); i_dim++) {
            rank_eval *= p_parameter[index[i_dim]];
            p_parameter += this->shape_[i_dim];
        }
        result += rank_eval;
    }
    return result;
}

// Check if these is a negative parameter in the model
__cuhostdev__ bool candy::Model::all_positive(void) const noexcept {
    return std::all_of(this->parameters_.cbegin(), this->parameters_.cend(),
                       [](const double & value) { return value >= 0; });
}

#ifdef __NVCC__

// Copy to shared memory by CUDA block
__cudevice__ void * candy::Model::copy_by_block(candy::Model * dest_ptr, void * parameters_data_ptr,
                                                std::uint64_t thread_idx, std::uint64_t block_size) const {
    // shallow copy of data pointer
    double * parameters_data = reinterpret_cast<double *>(parameters_data_ptr);
    std::uint64_t num_params = this->num_params(), ndim = this->ndim();
    if (thread_idx == 0) {
        dest_ptr->parameters_.assign(parameters_data, num_params);
        dest_ptr->shape_.resize(ndim);
        dest_ptr->rank_ = this->rank_;
        dest_ptr->rstride_ = this->rstride_;
        dest_ptr->offset_.resize(ndim);
    }
    __syncthreads();
    // copy data of parameters, shape and offset
    for (std::uint64_t i_param = thread_idx; i_param < num_params; i_param += block_size) {
        dest_ptr->parameters_[i_param] = this->parameters_[i_param];
    }
    __syncthreads();
    for (std::uint64_t i_dim = thread_idx; i_dim < ndim; i_dim += block_size) {
        dest_ptr->shape_[i_dim] = this->shape_[i_dim];
        dest_ptr->offset_[i_dim] = this->offset_[i_dim];
    }
    __syncthreads();
    return reinterpret_cast<void *>(parameters_data + num_params);
}

// Copy to shared memory by CUDA thread
__cudevice__ void * candy::Model::copy_by_thread(candy::Model * dest_ptr, void * parameters_data_ptr) const {
    // shallow copy of grid node, grid shape and grid vectors
    double * parameters_data = reinterpret_cast<double *>(parameters_data_ptr);
    std::uint64_t num_params = this->num_params(), ndim = this->ndim();
    dest_ptr->parameters_.assign(parameters_data, num_params);
    dest_ptr->shape_.resize(ndim);
    dest_ptr->rank_ = this->rank_;
    dest_ptr->rstride_ = this->rstride_;
    dest_ptr->offset_.resize(ndim);
    // copy data of grid nodes and grid shape
    for (std::uint64_t i_param = 0; i_param < num_params; i_param++) {
        dest_ptr->parameters_[i_param] = this->parameters_[i_param];
    }
    for (std::uint64_t i_dim = 0; i_dim < ndim; i_dim++) {
        dest_ptr->shape_[i_dim] = this->shape_[i_dim];
        dest_ptr->offset_[i_dim] = this->offset_[i_dim];
    }
    // re-calculate grid vectors
    return reinterpret_cast<void *>(parameters_data + num_params);
}

#endif  // __NVCC__

}  // namespace merlin
