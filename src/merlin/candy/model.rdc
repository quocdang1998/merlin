// Copyright 2023 quocdang1998
#include "merlin/candy/model.hpp"

#include "merlin/logger.hpp"  // CUHDERR
#include "merlin/utils.hpp"   // merlin::flatten_thread_index, merlin::size_of_block

namespace merlin {

// Get shape
__cuhostdev__ intvec candy::Model::get_model_shape(std::uint64_t * data_ptr) const noexcept {
    intvec result;
    if (data_ptr != nullptr) {
        result.assign(data_ptr, this->ndim());
    } else {
        result = intvec(this->parameters_.size());
    }
    for (std::uint64_t i_dim = 0; i_dim < this->ndim(); i_dim++) {
        result[i_dim] = this->parameters_[i_dim].size();
    }
    return result;
}

// Number of elements
__cuhostdev__ std::uint64_t candy::Model::size(void) const noexcept {
    std::uint64_t size = 0;
    for (std::uint64_t i_dim = 0; i_dim < this->ndim(); i_dim++) {
        size += this->parameters_[i_dim].size();
    }
    return size;
}

// Get dimension and index from contiguous index
__cuhostdev__ std::array<std::uint64_t, 2> candy::Model::convert_contiguous(std::uint64_t index) const noexcept {
    std::uint64_t dimension = 0;
    std::int64_t index_ = index;
    for (std::uint64_t i_dim = 0; i_dim < this->parameters_.size(); i_dim++) {
        index_ -= this->parameters_[i_dim].size();
        dimension += (index_ >= 0) ? 1 : 0;
        index = (index_ >= 0) ? index_ : index;
    }
    return {dimension, index};
}

// Evaluate result of the model at a given index
__cuhostdev__ double candy::Model::eval(const intvec & index) const noexcept {
    double result = 0.0;
    for (std::uint64_t r = 0; r < this->rank_; r++) {
        double rank_eval = 1.0;
        for (std::uint64_t i_dim = 0; i_dim < this->ndim(); i_dim++) {
            rank_eval *= this->get(i_dim, index[i_dim], r);
        }
        result += rank_eval;
    }
    return result;
}

#ifdef __NVCC__

// Copy to shared memory by CUDA block
__cudevice__ void * candy::Model::copy_by_block(candy::Model * dest_ptr, void * parameters_data_ptr,
                                                std::uint64_t thread_idx, std::uint64_t block_size) const {
    // copy of parameter vector pointer and rank
    floatvec * parameters_data = reinterpret_cast<floatvec *>(parameters_data_ptr);
    if (thread_idx == 0) {
        dest_ptr->parameters_.data() = parameters_data;
        dest_ptr->parameters_.size() = this->ndim();
        dest_ptr->rank_ = this->rank_;
    }
    __syncthreads();
    // copy parameter value
    void * floatvecs_data = reinterpret_cast<void *>(parameters_data + this->ndim());
    for (std::uint64_t i_dim = 0; i_dim < dest_ptr->ndim(); i_dim++) {
        floatvecs_data =
            this->parameters_[i_dim].copy_by_block(parameters_data + i_dim, floatvecs_data, thread_idx, block_size);
    }
    return floatvecs_data;
}

// Copy to shared memory by CUDA thread
__cudevice__ void * candy::Model::copy_by_thread(candy::Model * dest_ptr, void * parameters_data_ptr) const {
    // copy of parameters vector
    floatvec * parameters_data = reinterpret_cast<floatvec *>(parameters_data_ptr);
    dest_ptr->parameters_.data() = parameters_data;
    dest_ptr->parameters_.size() = this->ndim();
    dest_ptr->rank_ = this->rank_;
    // copy data of each parameter vector
    void * floatvecs_data = reinterpret_cast<void *>(parameters_data + this->ndim());
    for (std::uint64_t i_dim = 0; i_dim < dest_ptr->ndim(); i_dim++) {
        floatvecs_data = this->parameters_[i_dim].copy_by_thread(parameters_data + i_dim, floatvecs_data);
    }
    return floatvecs_data;
}

#endif  // __NVCC__

}  // namespace merlin
