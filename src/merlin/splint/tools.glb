// Copyright 2022 quocdang1998
#include "merlin/splint/tools.hpp"

#include <array>  // std::array

#include "merlin/cuda/memory.hpp"            // merlin::cuda::copy_class_to_shared_mem
#include "merlin/cuda/stream.hpp"            // merlin::cuda::Stream
#include "merlin/splint/cartesian_grid.hpp"  // merlin::splint::CartesianGrid
#include "merlin/splint/intpl/linear.hpp"    // merlin::splint::intpl::construct_linear
#include "merlin/splint/intpl/lagrange.hpp"  // merlin::splint::intpl::construct_lagrange
#include "merlin/splint/intpl/newton.hpp"    // merlin::splint::intpl::construction_newton
#include "merlin/utils.hpp"                  // merlin::prod_elements, merlin::flatten_thread_index,
                                             // merlin::size_of_block

namespace merlin {

// ---------------------------------------------------------------------------------------------------------------------
// Construct coefficients
// ---------------------------------------------------------------------------------------------------------------------

// Kernel calculating coefficients
__global__ static void construct_coeff_kernel(double * coeff, const splint::CartesianGrid * p_grid,
                                              const Vector<splint::Method> * p_method) {
    // functor to coefficient construction methods
    static const std::array<splint::ConstructionMethod, 3> construction_funcs {
        splint::intpl::construct_linear,
        splint::intpl::construct_lagrange,
        splint::intpl::construction_newton
    };
    // get thread index and num threads
    std::uint64_t thread_idx = flatten_thread_index(), block_size = size_of_block();
    // copy data to dynamic shared memory
    extern __shared__ char dynamic_shmem[];
    auto [end_shr_ptr, grid_shr, method_shr] = cuda::copy_class_to_shared_mem(dynamic_shmem, *p_grid, *p_method);
    // assign variable to static share memory
    const intvec & shape = grid_shr->shape();
    __shared__ char static_shmem[5 * sizeof(std::uint64_t) + sizeof(unsigned int)];
    std::uint64_t & num_subsystem = *(reinterpret_cast<std::uint64_t *>(static_shmem));
    std::uint64_t & element_size = *(&num_subsystem + 1);
    std::uint64_t & subsystem_size = *(&element_size + 1);
    std::uint64_t & numthreads_subsystem = *(&subsystem_size + 1);
    std::uint64_t & num_groups = *(&numthreads_subsystem + 1);
    unsigned int & i_method = *(reinterpret_cast<unsigned int *>(&num_groups + 1));
    // initialization
    if (thread_idx == 0) {
        num_subsystem = 1;
        element_size = prod_elements(shape);
        subsystem_size = 0;
        numthreads_subsystem = 0;
        num_groups = 0;
        i_method = 0;
    }
    __syncthreads();
    // solve matrix for each dimension
    for (std::uint64_t i_dim = 0; i_dim < grid_shr->ndim(); i_dim++) {
        // calculate number of thread per groups
        if (thread_idx == 0) {
            subsystem_size = element_size;
            element_size /= shape[i_dim];
            i_method = static_cast<unsigned int>((*method_shr)[i_dim]);
            numthreads_subsystem = block_size / num_subsystem;
            if (numthreads_subsystem == 0) {
                num_groups = block_size;
                numthreads_subsystem = 1;
            } else {
                num_groups = num_subsystem;
            }
        }
        __syncthreads();
        // parallel subsystem over the number of groups
        std::uint64_t numthreads_subsystem_local = numthreads_subsystem;
        std::uint64_t thread_idx_in_group = thread_idx % numthreads_subsystem_local;
        std::uint64_t group_idx = thread_idx / numthreads_subsystem_local;
        // copy some variable to thread local memory
        std::uint64_t num_subsystem_thr = num_subsystem, num_groups_thr = num_groups;
        std::uint64_t subsystem_size_thr = subsystem_size;
        std::uint64_t element_size_thr = element_size;
        for (std::uint64_t i_subsystem = group_idx; i_subsystem < num_subsystem_thr; i_subsystem += num_groups_thr) {
            double * subsystem_start = coeff + i_subsystem * subsystem_size_thr;
            construction_funcs[i_method](subsystem_start, grid_shr->grid_vectors()[i_dim], shape[i_dim],
                                         element_size_thr, thread_idx_in_group, numthreads_subsystem_local);
        }
        __syncthreads();
        // update number of sub-system
        if (thread_idx == 0) {
            num_subsystem *= shape[i_dim];
        }
        __syncthreads();
    }
}

// Construct interpolation coefficients with GPU parallelism
void splint::construct_coeff_gpu(double * coeff, const splint::CartesianGrid * p_grid,
                                 const Vector<splint::Method> * p_method, std::uint64_t n_threads,
                                 std::uint64_t shared_mem_size, cuda::Stream * stream_ptr) noexcept {
    // create an asynchrnous CUDA stream
    ::cudaStream_t stream = reinterpret_cast<::cudaStream_t>(stream_ptr->get_stream_ptr());
    // launch calculation
    construct_coeff_kernel<<<1, n_threads, shared_mem_size, stream>>>(coeff, p_grid, p_method);
}

// ---------------------------------------------------------------------------------------------------------------------
// Evaluate interpolation
// ---------------------------------------------------------------------------------------------------------------------

}  // namespace merlin
