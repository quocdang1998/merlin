// Copyright 2022 quocdang1998
#include "merlin/splint/tools.hpp"

#include <array>  // std::array

#include "merlin/cuda/memory.hpp"            // merlin::cuda::copy_objects
#include "merlin/cuda/stream.hpp"            // merlin::cuda::Stream
#include "merlin/splint/intpl/lagrange.hpp"  // merlin::splint::intpl::construct_lagrange
#include "merlin/splint/intpl/linear.hpp"    // merlin::splint::intpl::construct_linear
#include "merlin/splint/intpl/newton.hpp"    // merlin::splint::intpl::construction_newton
#include "merlin/thread_divider.hpp"         // merlin::ThreadDivider
#include "merlin/utils.hpp"                  // merlin::flatten_thread_index, merlin::size_of_block,
                                             // merlin::increment_index

namespace merlin {

// ---------------------------------------------------------------------------------------------------------------------
// Construct coefficients
// ---------------------------------------------------------------------------------------------------------------------

// Kernel calculating coefficients
__global__ static void construct_coeff_kernel(double * coeff, const grid::CartesianGrid * p_grid,
                                              const std::array<unsigned int, max_dim> * p_method) {
    // functor to coefficient construction methods
    static const std::array<splint::ConstructionMethod, 3> construction_funcs{
        splint::intpl::construct_linear,
        splint::intpl::construct_lagrange,
        splint::intpl::construction_newton,
    };
    // get thread index and num threads
    std::uint64_t thread_idx = flatten_thread_index(), block_size = size_of_block();
    // copy data to dynamic shared memory
    extern __shared__ char dynamic_shmem[];
    auto [end_shr_ptr, grid_shr] = cuda::copy_objects(dynamic_shmem, *p_grid);
    std::array<unsigned int, max_dim> cache_method(*p_method);
    const Index & shape = grid_shr->shape();
    // initialization
    std::uint64_t num_subsystem = 1;
    std::uint64_t element_size = grid_shr->size();
    std::uint64_t subsystem_size = 0;
    // solve matrix for each dimension
    for (std::uint64_t i_dim = 0; i_dim < grid_shr->ndim(); i_dim++) {
        // calculate number of thread per groups
        subsystem_size = element_size;
        element_size /= shape[i_dim];
        // parallel subsystem over the number of groups
        ThreadDivider thr_grp(num_subsystem, thread_idx, block_size);
        std::uint64_t local_num_subsystem = num_subsystem;
        std::uint64_t local_subsystem_size = subsystem_size;
        std::uint64_t local_element_size = element_size;
        unsigned int i_method = static_cast<unsigned int>(cache_method[i_dim]);
        for (std::uint64_t i_task = thr_grp.group_idx; i_task < local_num_subsystem; i_task += thr_grp.num_groups) {
            double * subsystem_start = coeff + i_task * local_subsystem_size;
            construction_funcs[i_method](subsystem_start, grid_shr->grid_vectors()[i_dim], shape[i_dim],
                                         local_element_size, thr_grp.thread_idx_in_group, thr_grp.numthreads_pertask);
        }
        // update number of sub-system
        num_subsystem *= shape[i_dim];
        // force synchronize threads befor moving on to the next axis
        __syncthreads();
    }
}

// Construct interpolation coefficients with GPU parallelism
void splint::construct_coeff_gpu(double * coeff, const grid::CartesianGrid * p_grid,
                                 const std::array<unsigned int, max_dim> * p_method, std::uint64_t n_threads,
                                 std::uint64_t shared_mem_size, const cuda::Stream * stream_ptr) noexcept {
    // create an asynchrnous CUDA stream
    ::cudaStream_t stream = reinterpret_cast<::cudaStream_t>(stream_ptr->get_stream_ptr());
    // launch calculation
    construct_coeff_kernel<<<1, n_threads, shared_mem_size, stream>>>(coeff, p_grid, p_method);
}

// ---------------------------------------------------------------------------------------------------------------------
// Evaluate interpolation
// ---------------------------------------------------------------------------------------------------------------------

// Kernel evaluating interpolation
__global__ static void eval_intpl_kernel(double * coeff, const grid::CartesianGrid * p_grid,
                                         const std::array<unsigned int, max_dim> * p_method, double * points,
                                         std::uint64_t n_points, double * result) {
    // get thread index and num threads
    std::uint64_t thread_idx = flatten_thread_index(), block_size = size_of_block();
    // copy to share memory
    extern __shared__ char dynamic_shmem[];
    auto [end_shr_ptr, grid_shr] = cuda::copy_objects(dynamic_shmem, *p_grid);
    std::array<unsigned int, max_dim> cache_method(*p_method);
    // initialization
    Index loop_index;
    loop_index.fill(0);
    Point cache, point;
    cache.fill(0.0);
    point.fill(0.0);
    std::uint64_t ndim = p_grid->ndim();
    // parallel calculation for each point
    std::uint64_t grid_size = grid_shr->size();
    const Index & shape = grid_shr->shape();
    for (std::uint64_t i_point = thread_idx; i_point < n_points; i_point += block_size) {
        // copy point coordinate to cache memory
        for (std::uint64_t i_dim = 0; i_dim < ndim; i_dim++) {
            point[i_dim] = points[i_point * ndim + i_dim];
        }
        // set last updated dimension and contiguous index
        std::int64_t last_updated_dim = ndim - 1;
        std::uint64_t contiguous_index = 0;
        // loop on each index and save evaluation by each coefficient to the cache array
        do {
            splint::recursive_interpolate(coeff, grid_size, contiguous_index, loop_index.data(), cache.data(),
                                          point.data(), last_updated_dim, shape.data(), grid_shr->grid_vectors().data(),
                                          cache_method, grid_shr->ndim());
            last_updated_dim = increment_index(loop_index.data(), shape.data(), ndim);
            contiguous_index++;
        } while (last_updated_dim != -1);
        // perform one last iteration on the last coefficient
        splint::recursive_interpolate(coeff, grid_size, contiguous_index, loop_index.data(), cache.data(), point.data(),
                                      0, shape.data(), grid_shr->grid_vectors().data(), cache_method, grid_shr->ndim());
        // save result and reset the cache
        result[i_point] = cache[0];
        cache[0] = 0.0;
    }
}

// Evaluate interpolation with GPU parallelism
void splint::eval_intpl_gpu(double * coeff, const grid::CartesianGrid * p_grid,
                            const std::array<unsigned int, max_dim> * p_method, double * points, std::uint64_t n_points,
                            double * result, std::uint64_t n_threads, std::uint64_t ndim, std::uint64_t shared_mem_size,
                            const cuda::Stream * stream_ptr) noexcept {
    // create an asynchrnous CUDA stream
    ::cudaStream_t stream = reinterpret_cast<::cudaStream_t>(stream_ptr->get_stream_ptr());
    // launch calculation
    eval_intpl_kernel<<<1, n_threads, shared_mem_size, stream>>>(coeff, p_grid, p_method, points, n_points, result);
}

}  // namespace merlin
