// Copyright 2022 quocdang1998
#include "merlin/splint/tools.hpp"

#include <array>  // std::array

#include "merlin/cuda/memory.hpp"            // merlin::cuda::copy_class_to_shared_mem
#include "merlin/cuda/stream.hpp"            // merlin::cuda::Stream
#include "merlin/splint/cartesian_grid.hpp"  // merlin::splint::CartesianGrid
#include "merlin/splint/intpl/linear.hpp"    // merlin::splint::intpl::construct_linear
#include "merlin/splint/intpl/lagrange.hpp"  // merlin::splint::intpl::construct_lagrange
#include "merlin/splint/intpl/newton.hpp"    // merlin::splint::intpl::construction_newton
#include "merlin/utils.hpp"                  // merlin::prod_elements, merlin::flatten_thread_index,
                                             // merlin::size_of_block

namespace merlin {

// ---------------------------------------------------------------------------------------------------------------------
// Construct coefficients
// ---------------------------------------------------------------------------------------------------------------------

// Kernel calculating coefficients
__global__ static void construct_coeff_kernel(double * coeff, const splint::CartesianGrid * p_grid,
                                              const Vector<splint::Method> * p_method) {
    // functor to coefficient construction methods
    static const std::array<splint::ConstructionMethod, 3> construction_funcs {
        splint::intpl::construct_linear,
        splint::intpl::construct_lagrange,
        splint::intpl::construction_newton
    };
    // get thread index and num threads
    std::uint64_t thread_idx = flatten_thread_index(), block_size = size_of_block();
    // copy data to dynamic shared memory
    extern __shared__ char dynamic_shmem[];
    auto [end_shr_ptr, grid_shr, method_shr] = cuda::copy_class_to_shared_mem(dynamic_shmem, *p_grid, *p_method);
    // assign variable to static share memory
    const intvec & shape = grid_shr->shape();
    __shared__ char static_shmem[5 * sizeof(std::uint64_t) + sizeof(unsigned int)];
    std::uint64_t & num_subsystem = *(reinterpret_cast<std::uint64_t *>(static_shmem));
    std::uint64_t & element_size = *(&num_subsystem + 1);
    std::uint64_t & subsystem_size = *(&element_size + 1);
    std::uint64_t & numthreads_subsystem = *(&subsystem_size + 1);
    std::uint64_t & num_groups = *(&numthreads_subsystem + 1);
    unsigned int & i_method = *(reinterpret_cast<unsigned int *>(&num_groups + 1));
    // initialization
    if (thread_idx == 0) {
        num_subsystem = 1;
        element_size = prod_elements(shape);
        subsystem_size = 0;
        numthreads_subsystem = 0;
        num_groups = 0;
        i_method = 0;
    }
    __syncthreads();
    // solve matrix for each dimension
    for (std::uint64_t i_dim = 0; i_dim < grid_shr->ndim(); i_dim++) {
        // calculate number of thread per groups
        if (thread_idx == 0) {
            subsystem_size = element_size;
            element_size /= shape[i_dim];
            i_method = static_cast<unsigned int>((*method_shr)[i_dim]);
            numthreads_subsystem = block_size / num_subsystem;
            if (numthreads_subsystem == 0) {
                num_groups = block_size;
                numthreads_subsystem = 1;
            } else {
                num_groups = num_subsystem;
            }
        }
        __syncthreads();
        // parallel subsystem over the number of groups
        std::uint64_t numthreads_subsystem_local = numthreads_subsystem;
        std::uint64_t thread_idx_in_group = thread_idx % numthreads_subsystem_local;
        std::uint64_t group_idx = thread_idx / numthreads_subsystem_local;
        // copy some variable to thread local memory
        std::uint64_t num_subsystem_thr = num_subsystem, num_groups_thr = num_groups;
        std::uint64_t subsystem_size_thr = subsystem_size;
        std::uint64_t element_size_thr = element_size;
        for (std::uint64_t i_subsystem = group_idx; i_subsystem < num_subsystem_thr; i_subsystem += num_groups_thr) {
            double * subsystem_start = coeff + i_subsystem * subsystem_size_thr;
            construction_funcs[i_method](subsystem_start, grid_shr->grid_vectors()[i_dim], shape[i_dim],
                                         element_size_thr, thread_idx_in_group, numthreads_subsystem_local);
        }
        __syncthreads();
        // update number of sub-system
        if (thread_idx == 0) {
            num_subsystem *= shape[i_dim];
        }
        __syncthreads();
    }
}

// Construct interpolation coefficients with GPU parallelism
void splint::construct_coeff_gpu(double * coeff, const splint::CartesianGrid * p_grid,
                                 const Vector<splint::Method> * p_method, std::uint64_t n_threads,
                                 std::uint64_t shared_mem_size, cuda::Stream * stream_ptr) noexcept {
    // create an asynchrnous CUDA stream
    ::cudaStream_t stream = reinterpret_cast<::cudaStream_t>(stream_ptr->get_stream_ptr());
    // launch calculation
    construct_coeff_kernel<<<1, n_threads, shared_mem_size, stream>>>(coeff, p_grid, p_method);
}

// ---------------------------------------------------------------------------------------------------------------------
// Evaluate interpolation
// ---------------------------------------------------------------------------------------------------------------------

// Kernel evaluating interpolation
__global__ static void eval_intpl_kernel(double * coeff, const splint::CartesianGrid * p_grid,
                                         const Vector<splint::Method> * p_method, double * points,
                                         std::uint64_t n_points, double * result) {
    // get thread index and num threads
    std::uint64_t thread_idx = flatten_thread_index(), block_size = size_of_block();
    // copy to share memory
    extern __shared__ char dynamic_shmem[];
    auto [end_shr_ptr, grid_shr, method_shr] = cuda::copy_class_to_shared_mem(dynamic_shmem, *p_grid, *p_method);
    if (thread_idx == 0) {
        std::printf("share mem: %p, grid_ptr: %p, method_ptr: %p, available: %p\n", dynamic_shmem, grid_shr, method_shr, end_shr_ptr);
    }
    // assigning + initializing the index vector and cache vector
    std::uint64_t ndim = grid_shr->ndim();
    std::uintptr_t free_shared_mem = reinterpret_cast<std::uintptr_t>(end_shr_ptr);
    free_shared_mem += thread_idx * ndim * (sizeof(std::uint64_t) + sizeof(double));
    std::uint64_t * loop_index_data = reinterpret_cast<std::uint64_t *>(free_shared_mem);
    double * cache_data = reinterpret_cast<double *>(loop_index_data + ndim);
    std::printf("Loop index data: %p\n", loop_index_data);
    for (std::uint64_t i = 0; i < ndim; i++) {
        loop_index_data[i] = 0;
        cache_data[i] = 0.0;
    }
    std::printf("Pass\n");
    intvec loop_index;
    loop_index.assign(loop_index_data, ndim);
    floatvec cache;
    cache.assign(cache_data, ndim);
    // parallel calculation for each point
    std::uint64_t grid_size = grid_shr->size();
    for (std::uint64_t i_point = thread_idx; i_point < n_points; i_point += block_size) {
        const double * point_data = points + i_point * ndim;
        std::int64_t last_updated_dim = ndim-1;
        std::uint64_t contiguous_index = 0;
        // loop on each index and save evaluation by each coefficient to the cache array
        do {
            splint::recursive_interpolate(coeff, grid_size, contiguous_index, loop_index.data(), cache.data(),
                                          point_data, last_updated_dim, grid_shr->shape().data(),
                                          grid_shr->grid_vectors().data(), *method_shr, grid_shr->ndim());
            last_updated_dim = increment_index(loop_index, grid_shr->shape());
            last_updated_dim = last_updated_dim;
            contiguous_index++;
        } while (last_updated_dim != -1);
        // perform one last iteration on the last coefficient
        splint::recursive_interpolate(coeff, grid_size, contiguous_index, loop_index.data(), cache.data(),
                                      point_data, 0, grid_shr->shape().data(), grid_shr->grid_vectors().data(),
                                      *method_shr, grid_shr->ndim());
        // save result and reset the cache
        result[i_point] = cache[0];
        cache[0] = 0.0;
    }
}

// Evaluate interpolation with GPU parallelism
void splint::eval_intpl_gpu(double * coeff, const splint::CartesianGrid * p_grid,
                            const Vector<splint::Method> * p_method, double * points, std::uint64_t n_points,
                            double * result, std::uint64_t n_threads, std::uint64_t ndim, std::uint64_t shared_mem_size,
                            cuda::Stream * stream_ptr) noexcept {
    // create an asynchrnous CUDA stream
    ::cudaStream_t stream = reinterpret_cast<::cudaStream_t>(stream_ptr->get_stream_ptr());
    // launch calculation
    shared_mem_size += ndim * n_threads * (sizeof(double) + sizeof(std::uint64_t));
    std::printf("Shared memory: %llu\n", shared_mem_size);
    eval_intpl_kernel<<<1, n_threads, shared_mem_size, stream>>>(coeff, p_grid, p_method, points, n_points, result);
}

}  // namespace merlin
